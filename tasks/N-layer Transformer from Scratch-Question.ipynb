{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be201dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8197dd17",
   "metadata": {},
   "source": [
    "### Setting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd96a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "['When', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow,', 'And', 'dig', 'deep', 'trenches']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentence))\n",
    "print(test_sentence[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969706a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 5\n",
    "\n",
    "bigram = [(\n",
    "    test_sentence[i:i + SENTENCE_LENGTH],  # input\n",
    "    test_sentence[i + 1: i+ SENTENCE_LENGTH + 1]  # label\n",
    ") for i in range(len(test_sentence) - SENTENCE_LENGTH)]\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6153f5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty', 'winters', 'shall', 'besiege'],\n",
       "  ['forty', 'winters', 'shall', 'besiege', 'thy']),\n",
       " (['forty', 'winters', 'shall', 'besiege', 'thy'],\n",
       "  ['winters', 'shall', 'besiege', 'thy', 'brow,']),\n",
       " (['winters', 'shall', 'besiege', 'thy', 'brow,'],\n",
       "  ['shall', 'besiege', 'thy', 'brow,', 'And']),\n",
       " (['shall', 'besiege', 'thy', 'brow,', 'And'],\n",
       "  ['besiege', 'thy', 'brow,', 'And', 'dig']),\n",
       " (['besiege', 'thy', 'brow,', 'And', 'dig'],\n",
       "  ['thy', 'brow,', 'And', 'dig', 'deep']),\n",
       " (['thy', 'brow,', 'And', 'dig', 'deep'],\n",
       "  ['brow,', 'And', 'dig', 'deep', 'trenches']),\n",
       " (['brow,', 'And', 'dig', 'deep', 'trenches'],\n",
       "  ['And', 'dig', 'deep', 'trenches', 'in']),\n",
       " (['And', 'dig', 'deep', 'trenches', 'in'],\n",
       "  ['dig', 'deep', 'trenches', 'in', 'thy']),\n",
       " (['dig', 'deep', 'trenches', 'in', 'thy'],\n",
       "  ['deep', 'trenches', 'in', 'thy', \"beauty's\"]),\n",
       " (['deep', 'trenches', 'in', 'thy', \"beauty's\"],\n",
       "  ['trenches', 'in', 'thy', \"beauty's\", 'field,'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4a91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111272c",
   "metadata": {},
   "source": [
    "## ZeroLayerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e883dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2791384822.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [14]\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.embed = ## Fill this line\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ZeroLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, token_embed_size: int, hidden_size: int):\n",
    "        super(ZeroLayerTransformer, self).__init__()\n",
    "        self.n_word = vocab_size\n",
    "        self.token_embedding = nn.Embedding(self.n_word, token_embed_size)\n",
    "        \n",
    "        self.embed = ## Fill this line\n",
    "        self.unembed = ## Fill this line\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        token_embed = self.token_embedding(x)\n",
    "        \n",
    "        embed = self.embed(token_embed)\n",
    "        # embed = F.relu(embed)\n",
    "        \n",
    "        output = self.unembed(embed)\n",
    "        log_prob = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c36469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zerolayertransformer = ZeroLayerTransformer(\n",
    "    vocab_size=len(word_to_idx),\n",
    "    token_embed_size=100,\n",
    "    hidden_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9137df",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fd1f04b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model, epochs_num: int = 500):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs_num):\n",
    "        print('epoch: {}'.format(epoch + 1))\n",
    "        print('*' * 10)\n",
    "        running_loss = 0\n",
    "        for data in bigram:\n",
    "            words, labels = data\n",
    "\n",
    "            words = torch.LongTensor([word_to_idx[word] for word in words])\n",
    "            labels = torch.LongTensor([word_to_idx[label] for label in labels])\n",
    "\n",
    "            # forward\n",
    "            out = model(words)\n",
    "            # print(out)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75cebc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.237314\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.147413\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.058027\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 4.969051\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 4.880385\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 4.791939\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.703631\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.615388\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.527147\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.438856\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.350475\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.261977\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.173349\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.084593\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 3.995726\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 3.906780\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 3.817801\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 3.728847\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 3.639990\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 3.551305\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 3.462875\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 3.374782\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 3.287111\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 3.199941\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 3.113351\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 3.027416\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 2.942212\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 2.857813\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 2.774294\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 2.691734\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 2.610212\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 2.529812\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 2.450618\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 2.372717\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 2.296197\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 2.221143\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 2.147641\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 2.075772\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 2.005613\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 1.937237\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 1.870707\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 1.806082\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 1.743411\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 1.682733\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 1.624080\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 1.567474\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 1.512927\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 1.460442\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 1.410015\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 1.361630\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 1.315267\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 1.270896\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 1.228482\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 1.187982\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 1.149350\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 1.112535\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 1.077480\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 1.044129\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 1.012421\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 0.982293\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 0.953682\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 0.926525\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 0.900757\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 0.876315\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 0.853135\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 0.831157\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 0.810320\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 0.790564\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 0.771834\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 0.754074\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 0.737230\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 0.721253\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 0.706094\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 0.691705\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 0.678044\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 0.665068\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 0.652738\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 0.641015\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 0.629865\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 0.619254\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 0.609151\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 0.599525\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 0.590350\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 0.581598\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 0.573246\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 0.565270\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 0.557649\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.550362\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.543391\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.536718\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.530325\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.524198\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.518322\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.512684\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.507270\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.502068\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.497067\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.492257\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.487627\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.483169\n",
      "epoch: 101\n",
      "**********\n",
      "Loss: 0.478874\n",
      "epoch: 102\n",
      "**********\n",
      "Loss: 0.474733\n",
      "epoch: 103\n",
      "**********\n",
      "Loss: 0.470738\n",
      "epoch: 104\n",
      "**********\n",
      "Loss: 0.466883\n",
      "epoch: 105\n",
      "**********\n",
      "Loss: 0.463161\n",
      "epoch: 106\n",
      "**********\n",
      "Loss: 0.459565\n",
      "epoch: 107\n",
      "**********\n",
      "Loss: 0.456090\n",
      "epoch: 108\n",
      "**********\n",
      "Loss: 0.452728\n",
      "epoch: 109\n",
      "**********\n",
      "Loss: 0.449477\n",
      "epoch: 110\n",
      "**********\n",
      "Loss: 0.446329\n",
      "epoch: 111\n",
      "**********\n",
      "Loss: 0.443281\n",
      "epoch: 112\n",
      "**********\n",
      "Loss: 0.440328\n",
      "epoch: 113\n",
      "**********\n",
      "Loss: 0.437465\n",
      "epoch: 114\n",
      "**********\n",
      "Loss: 0.434690\n",
      "epoch: 115\n",
      "**********\n",
      "Loss: 0.431997\n",
      "epoch: 116\n",
      "**********\n",
      "Loss: 0.429384\n",
      "epoch: 117\n",
      "**********\n",
      "Loss: 0.426847\n",
      "epoch: 118\n",
      "**********\n",
      "Loss: 0.424383\n",
      "epoch: 119\n",
      "**********\n",
      "Loss: 0.421989\n",
      "epoch: 120\n",
      "**********\n",
      "Loss: 0.419661\n",
      "epoch: 121\n",
      "**********\n",
      "Loss: 0.417399\n",
      "epoch: 122\n",
      "**********\n",
      "Loss: 0.415197\n",
      "epoch: 123\n",
      "**********\n",
      "Loss: 0.413056\n",
      "epoch: 124\n",
      "**********\n",
      "Loss: 0.410971\n",
      "epoch: 125\n",
      "**********\n",
      "Loss: 0.408941\n",
      "epoch: 126\n",
      "**********\n",
      "Loss: 0.406964\n",
      "epoch: 127\n",
      "**********\n",
      "Loss: 0.405037\n",
      "epoch: 128\n",
      "**********\n",
      "Loss: 0.403159\n",
      "epoch: 129\n",
      "**********\n",
      "Loss: 0.401329\n",
      "epoch: 130\n",
      "**********\n",
      "Loss: 0.399543\n",
      "epoch: 131\n",
      "**********\n",
      "Loss: 0.397802\n",
      "epoch: 132\n",
      "**********\n",
      "Loss: 0.396102\n",
      "epoch: 133\n",
      "**********\n",
      "Loss: 0.394444\n",
      "epoch: 134\n",
      "**********\n",
      "Loss: 0.392824\n",
      "epoch: 135\n",
      "**********\n",
      "Loss: 0.391243\n",
      "epoch: 136\n",
      "**********\n",
      "Loss: 0.389698\n",
      "epoch: 137\n",
      "**********\n",
      "Loss: 0.388188\n",
      "epoch: 138\n",
      "**********\n",
      "Loss: 0.386713\n",
      "epoch: 139\n",
      "**********\n",
      "Loss: 0.385271\n",
      "epoch: 140\n",
      "**********\n",
      "Loss: 0.383860\n",
      "epoch: 141\n",
      "**********\n",
      "Loss: 0.382481\n",
      "epoch: 142\n",
      "**********\n",
      "Loss: 0.381132\n",
      "epoch: 143\n",
      "**********\n",
      "Loss: 0.379812\n",
      "epoch: 144\n",
      "**********\n",
      "Loss: 0.378520\n",
      "epoch: 145\n",
      "**********\n",
      "Loss: 0.377256\n",
      "epoch: 146\n",
      "**********\n",
      "Loss: 0.376018\n",
      "epoch: 147\n",
      "**********\n",
      "Loss: 0.374805\n",
      "epoch: 148\n",
      "**********\n",
      "Loss: 0.373618\n",
      "epoch: 149\n",
      "**********\n",
      "Loss: 0.372455\n",
      "epoch: 150\n",
      "**********\n",
      "Loss: 0.371315\n",
      "epoch: 151\n",
      "**********\n",
      "Loss: 0.370198\n",
      "epoch: 152\n",
      "**********\n",
      "Loss: 0.369103\n",
      "epoch: 153\n",
      "**********\n",
      "Loss: 0.368030\n",
      "epoch: 154\n",
      "**********\n",
      "Loss: 0.366977\n",
      "epoch: 155\n",
      "**********\n",
      "Loss: 0.365945\n",
      "epoch: 156\n",
      "**********\n",
      "Loss: 0.364933\n",
      "epoch: 157\n",
      "**********\n",
      "Loss: 0.363940\n",
      "epoch: 158\n",
      "**********\n",
      "Loss: 0.362966\n",
      "epoch: 159\n",
      "**********\n",
      "Loss: 0.362010\n",
      "epoch: 160\n",
      "**********\n",
      "Loss: 0.361071\n",
      "epoch: 161\n",
      "**********\n",
      "Loss: 0.360150\n",
      "epoch: 162\n",
      "**********\n",
      "Loss: 0.359246\n",
      "epoch: 163\n",
      "**********\n",
      "Loss: 0.358358\n",
      "epoch: 164\n",
      "**********\n",
      "Loss: 0.357487\n",
      "epoch: 165\n",
      "**********\n",
      "Loss: 0.356630\n",
      "epoch: 166\n",
      "**********\n",
      "Loss: 0.355789\n",
      "epoch: 167\n",
      "**********\n",
      "Loss: 0.354963\n",
      "epoch: 168\n",
      "**********\n",
      "Loss: 0.354151\n",
      "epoch: 169\n",
      "**********\n",
      "Loss: 0.353353\n",
      "epoch: 170\n",
      "**********\n",
      "Loss: 0.352569\n",
      "epoch: 171\n",
      "**********\n",
      "Loss: 0.351799\n",
      "epoch: 172\n",
      "**********\n",
      "Loss: 0.351041\n",
      "epoch: 173\n",
      "**********\n",
      "Loss: 0.350296\n",
      "epoch: 174\n",
      "**********\n",
      "Loss: 0.349564\n",
      "epoch: 175\n",
      "**********\n",
      "Loss: 0.348844\n",
      "epoch: 176\n",
      "**********\n",
      "Loss: 0.348135\n",
      "epoch: 177\n",
      "**********\n",
      "Loss: 0.347439\n",
      "epoch: 178\n",
      "**********\n",
      "Loss: 0.346753\n",
      "epoch: 179\n",
      "**********\n",
      "Loss: 0.346079\n",
      "epoch: 180\n",
      "**********\n",
      "Loss: 0.345416\n",
      "epoch: 181\n",
      "**********\n",
      "Loss: 0.344763\n",
      "epoch: 182\n",
      "**********\n",
      "Loss: 0.344120\n",
      "epoch: 183\n",
      "**********\n",
      "Loss: 0.343488\n",
      "epoch: 184\n",
      "**********\n",
      "Loss: 0.342866\n",
      "epoch: 185\n",
      "**********\n",
      "Loss: 0.342253\n",
      "epoch: 186\n",
      "**********\n",
      "Loss: 0.341650\n",
      "epoch: 187\n",
      "**********\n",
      "Loss: 0.341056\n",
      "epoch: 188\n",
      "**********\n",
      "Loss: 0.340471\n",
      "epoch: 189\n",
      "**********\n",
      "Loss: 0.339895\n",
      "epoch: 190\n",
      "**********\n",
      "Loss: 0.339327\n",
      "epoch: 191\n",
      "**********\n",
      "Loss: 0.338769\n",
      "epoch: 192\n",
      "**********\n",
      "Loss: 0.338218\n",
      "epoch: 193\n",
      "**********\n",
      "Loss: 0.337676\n",
      "epoch: 194\n",
      "**********\n",
      "Loss: 0.337141\n",
      "epoch: 195\n",
      "**********\n",
      "Loss: 0.336615\n",
      "epoch: 196\n",
      "**********\n",
      "Loss: 0.336096\n",
      "epoch: 197\n",
      "**********\n",
      "Loss: 0.335585\n",
      "epoch: 198\n",
      "**********\n",
      "Loss: 0.335081\n",
      "epoch: 199\n",
      "**********\n",
      "Loss: 0.334584\n",
      "epoch: 200\n",
      "**********\n",
      "Loss: 0.334094\n",
      "epoch: 201\n",
      "**********\n",
      "Loss: 0.333612\n",
      "epoch: 202\n",
      "**********\n",
      "Loss: 0.333136\n",
      "epoch: 203\n",
      "**********\n",
      "Loss: 0.332667\n",
      "epoch: 204\n",
      "**********\n",
      "Loss: 0.332204\n",
      "epoch: 205\n",
      "**********\n",
      "Loss: 0.331748\n",
      "epoch: 206\n",
      "**********\n",
      "Loss: 0.331298\n",
      "epoch: 207\n",
      "**********\n",
      "Loss: 0.330854\n",
      "epoch: 208\n",
      "**********\n",
      "Loss: 0.330416\n",
      "epoch: 209\n",
      "**********\n",
      "Loss: 0.329984\n",
      "epoch: 210\n",
      "**********\n",
      "Loss: 0.329558\n",
      "epoch: 211\n",
      "**********\n",
      "Loss: 0.329138\n",
      "epoch: 212\n",
      "**********\n",
      "Loss: 0.328724\n",
      "epoch: 213\n",
      "**********\n",
      "Loss: 0.328315\n",
      "epoch: 214\n",
      "**********\n",
      "Loss: 0.327911\n",
      "epoch: 215\n",
      "**********\n",
      "Loss: 0.327513\n",
      "epoch: 216\n",
      "**********\n",
      "Loss: 0.327120\n",
      "epoch: 217\n",
      "**********\n",
      "Loss: 0.326732\n",
      "epoch: 218\n",
      "**********\n",
      "Loss: 0.326349\n",
      "epoch: 219\n",
      "**********\n",
      "Loss: 0.325971\n",
      "epoch: 220\n",
      "**********\n",
      "Loss: 0.325598\n",
      "epoch: 221\n",
      "**********\n",
      "Loss: 0.325230\n",
      "epoch: 222\n",
      "**********\n",
      "Loss: 0.324867\n",
      "epoch: 223\n",
      "**********\n",
      "Loss: 0.324508\n",
      "epoch: 224\n",
      "**********\n",
      "Loss: 0.324154\n",
      "epoch: 225\n",
      "**********\n",
      "Loss: 0.323804\n",
      "epoch: 226\n",
      "**********\n",
      "Loss: 0.323459\n",
      "epoch: 227\n",
      "**********\n",
      "Loss: 0.323118\n",
      "epoch: 228\n",
      "**********\n",
      "Loss: 0.322781\n",
      "epoch: 229\n",
      "**********\n",
      "Loss: 0.322449\n",
      "epoch: 230\n",
      "**********\n",
      "Loss: 0.322120\n",
      "epoch: 231\n",
      "**********\n",
      "Loss: 0.321796\n",
      "epoch: 232\n",
      "**********\n",
      "Loss: 0.321476\n",
      "epoch: 233\n",
      "**********\n",
      "Loss: 0.321159\n",
      "epoch: 234\n",
      "**********\n",
      "Loss: 0.320847\n",
      "epoch: 235\n",
      "**********\n",
      "Loss: 0.320538\n",
      "epoch: 236\n",
      "**********\n",
      "Loss: 0.320233\n",
      "epoch: 237\n",
      "**********\n",
      "Loss: 0.319932\n",
      "epoch: 238\n",
      "**********\n",
      "Loss: 0.319634\n",
      "epoch: 239\n",
      "**********\n",
      "Loss: 0.319340\n",
      "epoch: 240\n",
      "**********\n",
      "Loss: 0.319050\n",
      "epoch: 241\n",
      "**********\n",
      "Loss: 0.318763\n",
      "epoch: 242\n",
      "**********\n",
      "Loss: 0.318479\n",
      "epoch: 243\n",
      "**********\n",
      "Loss: 0.318199\n",
      "epoch: 244\n",
      "**********\n",
      "Loss: 0.317922\n",
      "epoch: 245\n",
      "**********\n",
      "Loss: 0.317648\n",
      "epoch: 246\n",
      "**********\n",
      "Loss: 0.317378\n",
      "epoch: 247\n",
      "**********\n",
      "Loss: 0.317110\n",
      "epoch: 248\n",
      "**********\n",
      "Loss: 0.316846\n",
      "epoch: 249\n",
      "**********\n",
      "Loss: 0.316585\n",
      "epoch: 250\n",
      "**********\n",
      "Loss: 0.316327\n",
      "epoch: 251\n",
      "**********\n",
      "Loss: 0.316072\n",
      "epoch: 252\n",
      "**********\n",
      "Loss: 0.315820\n",
      "epoch: 253\n",
      "**********\n",
      "Loss: 0.315570\n",
      "epoch: 254\n",
      "**********\n",
      "Loss: 0.315324\n",
      "epoch: 255\n",
      "**********\n",
      "Loss: 0.315080\n",
      "epoch: 256\n",
      "**********\n",
      "Loss: 0.314839\n",
      "epoch: 257\n",
      "**********\n",
      "Loss: 0.314601\n",
      "epoch: 258\n",
      "**********\n",
      "Loss: 0.314366\n",
      "epoch: 259\n",
      "**********\n",
      "Loss: 0.314133\n",
      "epoch: 260\n",
      "**********\n",
      "Loss: 0.313903\n",
      "epoch: 261\n",
      "**********\n",
      "Loss: 0.313675\n",
      "epoch: 262\n",
      "**********\n",
      "Loss: 0.313450\n",
      "epoch: 263\n",
      "**********\n",
      "Loss: 0.313228\n",
      "epoch: 264\n",
      "**********\n",
      "Loss: 0.313008\n",
      "epoch: 265\n",
      "**********\n",
      "Loss: 0.312790\n",
      "epoch: 266\n",
      "**********\n",
      "Loss: 0.312575\n",
      "epoch: 267\n",
      "**********\n",
      "Loss: 0.312362\n",
      "epoch: 268\n",
      "**********\n",
      "Loss: 0.312151\n",
      "epoch: 269\n",
      "**********\n",
      "Loss: 0.311943\n",
      "epoch: 270\n",
      "**********\n",
      "Loss: 0.311737\n",
      "epoch: 271\n",
      "**********\n",
      "Loss: 0.311533\n",
      "epoch: 272\n",
      "**********\n",
      "Loss: 0.311331\n",
      "epoch: 273\n",
      "**********\n",
      "Loss: 0.311132\n",
      "epoch: 274\n",
      "**********\n",
      "Loss: 0.310934\n",
      "epoch: 275\n",
      "**********\n",
      "Loss: 0.310739\n",
      "epoch: 276\n",
      "**********\n",
      "Loss: 0.310546\n",
      "epoch: 277\n",
      "**********\n",
      "Loss: 0.310355\n",
      "epoch: 278\n",
      "**********\n",
      "Loss: 0.310166\n",
      "epoch: 279\n",
      "**********\n",
      "Loss: 0.309979\n",
      "epoch: 280\n",
      "**********\n",
      "Loss: 0.309794\n",
      "epoch: 281\n",
      "**********\n",
      "Loss: 0.309611\n",
      "epoch: 282\n",
      "**********\n",
      "Loss: 0.309429\n",
      "epoch: 283\n",
      "**********\n",
      "Loss: 0.309250\n",
      "epoch: 284\n",
      "**********\n",
      "Loss: 0.309072\n",
      "epoch: 285\n",
      "**********\n",
      "Loss: 0.308897\n",
      "epoch: 286\n",
      "**********\n",
      "Loss: 0.308723\n",
      "epoch: 287\n",
      "**********\n",
      "Loss: 0.308551\n",
      "epoch: 288\n",
      "**********\n",
      "Loss: 0.308380\n",
      "epoch: 289\n",
      "**********\n",
      "Loss: 0.308212\n",
      "epoch: 290\n",
      "**********\n",
      "Loss: 0.308045\n",
      "epoch: 291\n",
      "**********\n",
      "Loss: 0.307880\n",
      "epoch: 292\n",
      "**********\n",
      "Loss: 0.307716\n",
      "epoch: 293\n",
      "**********\n",
      "Loss: 0.307554\n",
      "epoch: 294\n",
      "**********\n",
      "Loss: 0.307394\n",
      "epoch: 295\n",
      "**********\n",
      "Loss: 0.307235\n",
      "epoch: 296\n",
      "**********\n",
      "Loss: 0.307078\n",
      "epoch: 297\n",
      "**********\n",
      "Loss: 0.306922\n",
      "epoch: 298\n",
      "**********\n",
      "Loss: 0.306768\n",
      "epoch: 299\n",
      "**********\n",
      "Loss: 0.306616\n",
      "epoch: 300\n",
      "**********\n",
      "Loss: 0.306465\n",
      "epoch: 301\n",
      "**********\n",
      "Loss: 0.306315\n",
      "epoch: 302\n",
      "**********\n",
      "Loss: 0.306167\n",
      "epoch: 303\n",
      "**********\n",
      "Loss: 0.306020\n",
      "epoch: 304\n",
      "**********\n",
      "Loss: 0.305875\n",
      "epoch: 305\n",
      "**********\n",
      "Loss: 0.305731\n",
      "epoch: 306\n",
      "**********\n",
      "Loss: 0.305589\n",
      "epoch: 307\n",
      "**********\n",
      "Loss: 0.305448\n",
      "epoch: 308\n",
      "**********\n",
      "Loss: 0.305308\n",
      "epoch: 309\n",
      "**********\n",
      "Loss: 0.305170\n",
      "epoch: 310\n",
      "**********\n",
      "Loss: 0.305032\n",
      "epoch: 311\n",
      "**********\n",
      "Loss: 0.304897\n",
      "epoch: 312\n",
      "**********\n",
      "Loss: 0.304762\n",
      "epoch: 313\n",
      "**********\n",
      "Loss: 0.304629\n",
      "epoch: 314\n",
      "**********\n",
      "Loss: 0.304497\n",
      "epoch: 315\n",
      "**********\n",
      "Loss: 0.304366\n",
      "epoch: 316\n",
      "**********\n",
      "Loss: 0.304236\n",
      "epoch: 317\n",
      "**********\n",
      "Loss: 0.304108\n",
      "epoch: 318\n",
      "**********\n",
      "Loss: 0.303980\n",
      "epoch: 319\n",
      "**********\n",
      "Loss: 0.303854\n",
      "epoch: 320\n",
      "**********\n",
      "Loss: 0.303729\n",
      "epoch: 321\n",
      "**********\n",
      "Loss: 0.303605\n",
      "epoch: 322\n",
      "**********\n",
      "Loss: 0.303483\n",
      "epoch: 323\n",
      "**********\n",
      "Loss: 0.303361\n",
      "epoch: 324\n",
      "**********\n",
      "Loss: 0.303241\n",
      "epoch: 325\n",
      "**********\n",
      "Loss: 0.303121\n",
      "epoch: 326\n",
      "**********\n",
      "Loss: 0.303003\n",
      "epoch: 327\n",
      "**********\n",
      "Loss: 0.302885\n",
      "epoch: 328\n",
      "**********\n",
      "Loss: 0.302769\n",
      "epoch: 329\n",
      "**********\n",
      "Loss: 0.302654\n",
      "epoch: 330\n",
      "**********\n",
      "Loss: 0.302540\n",
      "epoch: 331\n",
      "**********\n",
      "Loss: 0.302426\n",
      "epoch: 332\n",
      "**********\n",
      "Loss: 0.302314\n",
      "epoch: 333\n",
      "**********\n",
      "Loss: 0.302203\n",
      "epoch: 334\n",
      "**********\n",
      "Loss: 0.302092\n",
      "epoch: 335\n",
      "**********\n",
      "Loss: 0.301983\n",
      "epoch: 336\n",
      "**********\n",
      "Loss: 0.301875\n",
      "epoch: 337\n",
      "**********\n",
      "Loss: 0.301767\n",
      "epoch: 338\n",
      "**********\n",
      "Loss: 0.301661\n",
      "epoch: 339\n",
      "**********\n",
      "Loss: 0.301555\n",
      "epoch: 340\n",
      "**********\n",
      "Loss: 0.301450\n",
      "epoch: 341\n",
      "**********\n",
      "Loss: 0.301346\n",
      "epoch: 342\n",
      "**********\n",
      "Loss: 0.301243\n",
      "epoch: 343\n",
      "**********\n",
      "Loss: 0.301141\n",
      "epoch: 344\n",
      "**********\n",
      "Loss: 0.301040\n",
      "epoch: 345\n",
      "**********\n",
      "Loss: 0.300939\n",
      "epoch: 346\n",
      "**********\n",
      "Loss: 0.300839\n",
      "epoch: 347\n",
      "**********\n",
      "Loss: 0.300741\n",
      "epoch: 348\n",
      "**********\n",
      "Loss: 0.300643\n",
      "epoch: 349\n",
      "**********\n",
      "Loss: 0.300545\n",
      "epoch: 350\n",
      "**********\n",
      "Loss: 0.300449\n",
      "epoch: 351\n",
      "**********\n",
      "Loss: 0.300353\n",
      "epoch: 352\n",
      "**********\n",
      "Loss: 0.300258\n",
      "epoch: 353\n",
      "**********\n",
      "Loss: 0.300164\n",
      "epoch: 354\n",
      "**********\n",
      "Loss: 0.300071\n",
      "epoch: 355\n",
      "**********\n",
      "Loss: 0.299978\n",
      "epoch: 356\n",
      "**********\n",
      "Loss: 0.299886\n",
      "epoch: 357\n",
      "**********\n",
      "Loss: 0.299795\n",
      "epoch: 358\n",
      "**********\n",
      "Loss: 0.299705\n",
      "epoch: 359\n",
      "**********\n",
      "Loss: 0.299615\n",
      "epoch: 360\n",
      "**********\n",
      "Loss: 0.299526\n",
      "epoch: 361\n",
      "**********\n",
      "Loss: 0.299438\n",
      "epoch: 362\n",
      "**********\n",
      "Loss: 0.299350\n",
      "epoch: 363\n",
      "**********\n",
      "Loss: 0.299263\n",
      "epoch: 364\n",
      "**********\n",
      "Loss: 0.299177\n",
      "epoch: 365\n",
      "**********\n",
      "Loss: 0.299091\n",
      "epoch: 366\n",
      "**********\n",
      "Loss: 0.299006\n",
      "epoch: 367\n",
      "**********\n",
      "Loss: 0.298922\n",
      "epoch: 368\n",
      "**********\n",
      "Loss: 0.298839\n",
      "epoch: 369\n",
      "**********\n",
      "Loss: 0.298756\n",
      "epoch: 370\n",
      "**********\n",
      "Loss: 0.298673\n",
      "epoch: 371\n",
      "**********\n",
      "Loss: 0.298591\n",
      "epoch: 372\n",
      "**********\n",
      "Loss: 0.298510\n",
      "epoch: 373\n",
      "**********\n",
      "Loss: 0.298430\n",
      "epoch: 374\n",
      "**********\n",
      "Loss: 0.298350\n",
      "epoch: 375\n",
      "**********\n",
      "Loss: 0.298271\n",
      "epoch: 376\n",
      "**********\n",
      "Loss: 0.298192\n",
      "epoch: 377\n",
      "**********\n",
      "Loss: 0.298114\n",
      "epoch: 378\n",
      "**********\n",
      "Loss: 0.298036\n",
      "epoch: 379\n",
      "**********\n",
      "Loss: 0.297959\n",
      "epoch: 380\n",
      "**********\n",
      "Loss: 0.297883\n",
      "epoch: 381\n",
      "**********\n",
      "Loss: 0.297807\n",
      "epoch: 382\n",
      "**********\n",
      "Loss: 0.297731\n",
      "epoch: 383\n",
      "**********\n",
      "Loss: 0.297657\n",
      "epoch: 384\n",
      "**********\n",
      "Loss: 0.297582\n",
      "epoch: 385\n",
      "**********\n",
      "Loss: 0.297509\n",
      "epoch: 386\n",
      "**********\n",
      "Loss: 0.297436\n",
      "epoch: 387\n",
      "**********\n",
      "Loss: 0.297363\n",
      "epoch: 388\n",
      "**********\n",
      "Loss: 0.297291\n",
      "epoch: 389\n",
      "**********\n",
      "Loss: 0.297219\n",
      "epoch: 390\n",
      "**********\n",
      "Loss: 0.297148\n",
      "epoch: 391\n",
      "**********\n",
      "Loss: 0.297077\n",
      "epoch: 392\n",
      "**********\n",
      "Loss: 0.297007\n",
      "epoch: 393\n",
      "**********\n",
      "Loss: 0.296938\n",
      "epoch: 394\n",
      "**********\n",
      "Loss: 0.296869\n",
      "epoch: 395\n",
      "**********\n",
      "Loss: 0.296800\n",
      "epoch: 396\n",
      "**********\n",
      "Loss: 0.296732\n",
      "epoch: 397\n",
      "**********\n",
      "Loss: 0.296664\n",
      "epoch: 398\n",
      "**********\n",
      "Loss: 0.296597\n",
      "epoch: 399\n",
      "**********\n",
      "Loss: 0.296530\n",
      "epoch: 400\n",
      "**********\n",
      "Loss: 0.296464\n",
      "epoch: 401\n",
      "**********\n",
      "Loss: 0.296398\n",
      "epoch: 402\n",
      "**********\n",
      "Loss: 0.296333\n",
      "epoch: 403\n",
      "**********\n",
      "Loss: 0.296268\n",
      "epoch: 404\n",
      "**********\n",
      "Loss: 0.296203\n",
      "epoch: 405\n",
      "**********\n",
      "Loss: 0.296139\n",
      "epoch: 406\n",
      "**********\n",
      "Loss: 0.296075\n",
      "epoch: 407\n",
      "**********\n",
      "Loss: 0.296012\n",
      "epoch: 408\n",
      "**********\n",
      "Loss: 0.295949\n",
      "epoch: 409\n",
      "**********\n",
      "Loss: 0.295887\n",
      "epoch: 410\n",
      "**********\n",
      "Loss: 0.295825\n",
      "epoch: 411\n",
      "**********\n",
      "Loss: 0.295763\n",
      "epoch: 412\n",
      "**********\n",
      "Loss: 0.295702\n",
      "epoch: 413\n",
      "**********\n",
      "Loss: 0.295642\n",
      "epoch: 414\n",
      "**********\n",
      "Loss: 0.295581\n",
      "epoch: 415\n",
      "**********\n",
      "Loss: 0.295521\n",
      "epoch: 416\n",
      "**********\n",
      "Loss: 0.295462\n",
      "epoch: 417\n",
      "**********\n",
      "Loss: 0.295402\n",
      "epoch: 418\n",
      "**********\n",
      "Loss: 0.295344\n",
      "epoch: 419\n",
      "**********\n",
      "Loss: 0.295285\n",
      "epoch: 420\n",
      "**********\n",
      "Loss: 0.295227\n",
      "epoch: 421\n",
      "**********\n",
      "Loss: 0.295170\n",
      "epoch: 422\n",
      "**********\n",
      "Loss: 0.295112\n",
      "epoch: 423\n",
      "**********\n",
      "Loss: 0.295055\n",
      "epoch: 424\n",
      "**********\n",
      "Loss: 0.294999\n",
      "epoch: 425\n",
      "**********\n",
      "Loss: 0.294943\n",
      "epoch: 426\n",
      "**********\n",
      "Loss: 0.294887\n",
      "epoch: 427\n",
      "**********\n",
      "Loss: 0.294831\n",
      "epoch: 428\n",
      "**********\n",
      "Loss: 0.294776\n",
      "epoch: 429\n",
      "**********\n",
      "Loss: 0.294721\n",
      "epoch: 430\n",
      "**********\n",
      "Loss: 0.294667\n",
      "epoch: 431\n",
      "**********\n",
      "Loss: 0.294613\n",
      "epoch: 432\n",
      "**********\n",
      "Loss: 0.294559\n",
      "epoch: 433\n",
      "**********\n",
      "Loss: 0.294505\n",
      "epoch: 434\n",
      "**********\n",
      "Loss: 0.294452\n",
      "epoch: 435\n",
      "**********\n",
      "Loss: 0.294400\n",
      "epoch: 436\n",
      "**********\n",
      "Loss: 0.294347\n",
      "epoch: 437\n",
      "**********\n",
      "Loss: 0.294295\n",
      "epoch: 438\n",
      "**********\n",
      "Loss: 0.294243\n",
      "epoch: 439\n",
      "**********\n",
      "Loss: 0.294192\n",
      "epoch: 440\n",
      "**********\n",
      "Loss: 0.294140\n",
      "epoch: 441\n",
      "**********\n",
      "Loss: 0.294089\n",
      "epoch: 442\n",
      "**********\n",
      "Loss: 0.294039\n",
      "epoch: 443\n",
      "**********\n",
      "Loss: 0.293989\n",
      "epoch: 444\n",
      "**********\n",
      "Loss: 0.293939\n",
      "epoch: 445\n",
      "**********\n",
      "Loss: 0.293889\n",
      "epoch: 446\n",
      "**********\n",
      "Loss: 0.293840\n",
      "epoch: 447\n",
      "**********\n",
      "Loss: 0.293790\n",
      "epoch: 448\n",
      "**********\n",
      "Loss: 0.293742\n",
      "epoch: 449\n",
      "**********\n",
      "Loss: 0.293693\n",
      "epoch: 450\n",
      "**********\n",
      "Loss: 0.293645\n",
      "epoch: 451\n",
      "**********\n",
      "Loss: 0.293597\n",
      "epoch: 452\n",
      "**********\n",
      "Loss: 0.293549\n",
      "epoch: 453\n",
      "**********\n",
      "Loss: 0.293502\n",
      "epoch: 454\n",
      "**********\n",
      "Loss: 0.293455\n",
      "epoch: 455\n",
      "**********\n",
      "Loss: 0.293408\n",
      "epoch: 456\n",
      "**********\n",
      "Loss: 0.293361\n",
      "epoch: 457\n",
      "**********\n",
      "Loss: 0.293315\n",
      "epoch: 458\n",
      "**********\n",
      "Loss: 0.293269\n",
      "epoch: 459\n",
      "**********\n",
      "Loss: 0.293223\n",
      "epoch: 460\n",
      "**********\n",
      "Loss: 0.293178\n",
      "epoch: 461\n",
      "**********\n",
      "Loss: 0.293133\n",
      "epoch: 462\n",
      "**********\n",
      "Loss: 0.293088\n",
      "epoch: 463\n",
      "**********\n",
      "Loss: 0.293043\n",
      "epoch: 464\n",
      "**********\n",
      "Loss: 0.292998\n",
      "epoch: 465\n",
      "**********\n",
      "Loss: 0.292954\n",
      "epoch: 466\n",
      "**********\n",
      "Loss: 0.292910\n",
      "epoch: 467\n",
      "**********\n",
      "Loss: 0.292867\n",
      "epoch: 468\n",
      "**********\n",
      "Loss: 0.292823\n",
      "epoch: 469\n",
      "**********\n",
      "Loss: 0.292780\n",
      "epoch: 470\n",
      "**********\n",
      "Loss: 0.292737\n",
      "epoch: 471\n",
      "**********\n",
      "Loss: 0.292694\n",
      "epoch: 472\n",
      "**********\n",
      "Loss: 0.292652\n",
      "epoch: 473\n",
      "**********\n",
      "Loss: 0.292609\n",
      "epoch: 474\n",
      "**********\n",
      "Loss: 0.292567\n",
      "epoch: 475\n",
      "**********\n",
      "Loss: 0.292525\n",
      "epoch: 476\n",
      "**********\n",
      "Loss: 0.292484\n",
      "epoch: 477\n",
      "**********\n",
      "Loss: 0.292443\n",
      "epoch: 478\n",
      "**********\n",
      "Loss: 0.292401\n",
      "epoch: 479\n",
      "**********\n",
      "Loss: 0.292361\n",
      "epoch: 480\n",
      "**********\n",
      "Loss: 0.292320\n",
      "epoch: 481\n",
      "**********\n",
      "Loss: 0.292279\n",
      "epoch: 482\n",
      "**********\n",
      "Loss: 0.292239\n",
      "epoch: 483\n",
      "**********\n",
      "Loss: 0.292199\n",
      "epoch: 484\n",
      "**********\n",
      "Loss: 0.292159\n",
      "epoch: 485\n",
      "**********\n",
      "Loss: 0.292120\n",
      "epoch: 486\n",
      "**********\n",
      "Loss: 0.292080\n",
      "epoch: 487\n",
      "**********\n",
      "Loss: 0.292041\n",
      "epoch: 488\n",
      "**********\n",
      "Loss: 0.292002\n",
      "epoch: 489\n",
      "**********\n",
      "Loss: 0.291963\n",
      "epoch: 490\n",
      "**********\n",
      "Loss: 0.291925\n",
      "epoch: 491\n",
      "**********\n",
      "Loss: 0.291887\n",
      "epoch: 492\n",
      "**********\n",
      "Loss: 0.291848\n",
      "epoch: 493\n",
      "**********\n",
      "Loss: 0.291810\n",
      "epoch: 494\n",
      "**********\n",
      "Loss: 0.291773\n",
      "epoch: 495\n",
      "**********\n",
      "Loss: 0.291735\n",
      "epoch: 496\n",
      "**********\n",
      "Loss: 0.291698\n",
      "epoch: 497\n",
      "**********\n",
      "Loss: 0.291661\n",
      "epoch: 498\n",
      "**********\n",
      "Loss: 0.291624\n",
      "epoch: 499\n",
      "**********\n",
      "Loss: 0.291587\n",
      "epoch: 500\n",
      "**********\n",
      "Loss: 0.291550\n"
     ]
    }
   ],
   "source": [
    "train(zerolayertransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c2be5",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01e46358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_score):\n",
    "    fig, axes = plt.subplots(1, len(attention_score))\n",
    "    for i, score in enumerate(attention_score):\n",
    "        img = axes[i].imshow(score)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, sample_idx: int, with_attention: bool = False):\n",
    "    words, labels = bigram[sample_idx]\n",
    "    words = torch.LongTensor([word_to_idx[word] for word in words])\n",
    "    \n",
    "    if with_attention:\n",
    "        out, attention_score = model(words, with_attention=True)\n",
    "        attention_score = attention_score.cpu().detach().numpy()\n",
    "        plot_attention(attention_score)\n",
    "    else:\n",
    "        out = model(words)\n",
    "\n",
    "    predicted_labels = torch.argmax(out, dim=-1)\n",
    "    predicted_words = [idx_to_word[prediction] for prediction in predicted_labels.tolist()]\n",
    "\n",
    "    print('real words are {}, predicted words are {}'.format(labels, predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a5e8c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['shall', 'besiege', 'thy', 'brow,', 'And'], predicted words are ['shall', 'besiege', 'thy', \"beauty's\", 'And']\n"
     ]
    }
   ],
   "source": [
    "# right answer case\n",
    "evaluate(zerolayertransformer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ae37e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['brow,', 'And', 'dig', 'deep', 'trenches'], predicted words are [\"beauty's\", 'And', 'see', 'deep', 'sunken']\n"
     ]
    }
   ],
   "source": [
    "# wrong answer case\n",
    "evaluate(zerolayertransformer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "388d186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why wrong?\n",
    "# bigram result with 'thy'\n",
    "\n",
    "# ('thy', 'brow,'),\n",
    "# ('thy', \"beauty's\")\n",
    "# ('Thy', \"youth's\")\n",
    "# ('thy', 'beauty'),\n",
    "# ('thy', 'lusty'),\n",
    "# ('thy', \"beauty's\")\n",
    "# ('thy', 'blood'),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4a86b",
   "metadata": {},
   "source": [
    "## OneLayerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c21375fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, heads_num: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % heads_num == 0\n",
    "        self.head_size = ## Fill this line \n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads_num = heads_num\n",
    "        \n",
    "        self.w_q = ## Fill this line \n",
    "        self.w_k = ## Fill this line \n",
    "        self.w_v = ## Fill this line \n",
    "        self.w_o = ## Fill this line \n",
    "    \n",
    "    def split_to_heads(self, item: Tensor) -> Tensor:\n",
    "        item_shape = item.shape[:-1] # B, S, D\n",
    "        item = item.reshape(\n",
    "            *item_shape, self.heads_num, self.head_size\n",
    "        ) # B, S, H_num, H_size\n",
    "        \n",
    "        item = item.transpose(-2, -3) # B, H_num, S, H_size\n",
    "        return item\n",
    "    \n",
    "    def combine_from_heads(self, item: Tensor) -> Tensor:\n",
    "        item = item.transpose(-2, -3) # B, S, H_num, H_size\n",
    "        item_shape = item.shape[:-2] \n",
    "        item = item.reshape(*item_shape, self.hidden_size)\n",
    "        return item\n",
    "    \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        q = self.split_to_heads(self.w_q(q)) # B, S, D -> B, H_num, S, H_size\n",
    "        k = self.split_to_heads(self.w_k(k)) # B, S, D -> B, H_num, S, H_size\n",
    "        v = self.split_to_heads(self.w_v(v)) # B, S, D -> B, H_num, S, H_size\n",
    " \n",
    "        k_transpose = torch.transpose(k, -2, -1)\n",
    "        \n",
    "        attention_map = q @ k_transpose / math.sqrt(self.head_size) # B, H_num, S, S\n",
    "        attention_score = F.softmax(attention_map, dim=-1)\n",
    "        \n",
    "        attention_result = attention_score @ v # B, H_num, S, H_size\n",
    "        attention_result = self.combine_from_heads(attention_result) # B, S, D\n",
    "        \n",
    "        return self.w_o(attention_result), attention_score # B, S, D\n",
    "\n",
    "class OneLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, token_embed_size: int, hidden_size: int, heads_num: int):\n",
    "        super(OneLayerTransformer, self).__init__()\n",
    "        self.n_word = vocab_size\n",
    "        self.token_embedding = nn.Embedding(self.n_word, token_embed_size)\n",
    "        self.embed = nn.Linear(token_embed_size, hidden_size)\n",
    "        self.unembed = nn.Linear(hidden_size, self.n_word)\n",
    "        \n",
    "        self.mha = MultiHeadAttention( hidden_size=hidden_size, heads_num=heads_num)\n",
    "        \n",
    "    def forward(self, x: Tensor, with_attention: bool = False) -> Tensor:\n",
    "        token_embed = self.token_embedding(x)\n",
    "        embed = self.embed(token_embed)\n",
    "        embed = F.relu(embed)\n",
    "        \n",
    "        x, attention_score = self.mha(embed, embed, embed)\n",
    "        x1 = ## Fill this line # residual connection\n",
    "        output = self.unembed(x1)\n",
    "        log_prob = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        if with_attention:\n",
    "            return log_prob, attention_score\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd736792",
   "metadata": {},
   "outputs": [],
   "source": [
    "onelayertransformer = OneLayerTransformer(\n",
    "    vocab_size=len(word_to_idx),\n",
    "    token_embed_size=100,\n",
    "    hidden_size=128,\n",
    "    heads_num=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d00caa58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.198430\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.157236\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.116435\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.075919\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.035645\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 4.995493\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.955404\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.915423\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.875515\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.835665\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.795848\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.756007\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.716089\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.676061\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 4.635898\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 4.595572\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 4.555035\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 4.514373\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 4.473463\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 4.432249\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 4.390792\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 4.349130\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 4.307187\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 4.264976\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 4.222484\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 4.179763\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 4.136755\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 4.093566\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 4.050177\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 4.006670\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 3.963071\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 3.919324\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 3.875466\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 3.831489\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 3.787444\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 3.743296\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 3.699077\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 3.654804\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 3.610469\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 3.565998\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 3.521379\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 3.476666\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 3.431818\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 3.386807\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 3.341647\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 3.296331\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 3.250820\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 3.205137\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 3.159255\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 3.113155\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 3.066883\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 3.020405\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 2.973735\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 2.926972\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 2.880080\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 2.833048\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 2.785914\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 2.738672\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 2.691362\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 2.643990\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 2.596563\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 2.549119\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 2.501696\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 2.454340\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 2.407062\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 2.359899\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 2.312876\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 2.266010\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 2.219382\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 2.172982\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 2.126871\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 2.081052\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 2.035582\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 1.990504\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 1.945831\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 1.901604\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 1.857872\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 1.814642\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 1.771951\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 1.729832\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 1.688317\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 1.647439\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 1.607201\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 1.567650\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 1.528824\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 1.490711\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 1.453366\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 1.416805\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 1.381024\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 1.346056\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 1.311890\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 1.278568\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 1.246092\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 1.214446\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 1.183639\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 1.153673\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 1.124542\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 1.096262\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 1.068801\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 1.042165\n",
      "epoch: 101\n",
      "**********\n",
      "Loss: 1.016355\n",
      "epoch: 102\n",
      "**********\n",
      "Loss: 0.991347\n",
      "epoch: 103\n",
      "**********\n",
      "Loss: 0.967136\n",
      "epoch: 104\n",
      "**********\n",
      "Loss: 0.943730\n",
      "epoch: 105\n",
      "**********\n",
      "Loss: 0.921076\n",
      "epoch: 106\n",
      "**********\n",
      "Loss: 0.899198\n",
      "epoch: 107\n",
      "**********\n",
      "Loss: 0.878049\n",
      "epoch: 108\n",
      "**********\n",
      "Loss: 0.857618\n",
      "epoch: 109\n",
      "**********\n",
      "Loss: 0.837904\n",
      "epoch: 110\n",
      "**********\n",
      "Loss: 0.818881\n",
      "epoch: 111\n",
      "**********\n",
      "Loss: 0.800515\n",
      "epoch: 112\n",
      "**********\n",
      "Loss: 0.782815\n",
      "epoch: 113\n",
      "**********\n",
      "Loss: 0.765714\n",
      "epoch: 114\n",
      "**********\n",
      "Loss: 0.749242\n",
      "epoch: 115\n",
      "**********\n",
      "Loss: 0.733366\n",
      "epoch: 116\n",
      "**********\n",
      "Loss: 0.718033\n",
      "epoch: 117\n",
      "**********\n",
      "Loss: 0.703276\n",
      "epoch: 118\n",
      "**********\n",
      "Loss: 0.689030\n",
      "epoch: 119\n",
      "**********\n",
      "Loss: 0.675296\n",
      "epoch: 120\n",
      "**********\n",
      "Loss: 0.662072\n",
      "epoch: 121\n",
      "**********\n",
      "Loss: 0.649284\n",
      "epoch: 122\n",
      "**********\n",
      "Loss: 0.636982\n",
      "epoch: 123\n",
      "**********\n",
      "Loss: 0.625102\n",
      "epoch: 124\n",
      "**********\n",
      "Loss: 0.613631\n",
      "epoch: 125\n",
      "**********\n",
      "Loss: 0.602562\n",
      "epoch: 126\n",
      "**********\n",
      "Loss: 0.591892\n",
      "epoch: 127\n",
      "**********\n",
      "Loss: 0.581572\n",
      "epoch: 128\n",
      "**********\n",
      "Loss: 0.571625\n",
      "epoch: 129\n",
      "**********\n",
      "Loss: 0.561990\n",
      "epoch: 130\n",
      "**********\n",
      "Loss: 0.552683\n",
      "epoch: 131\n",
      "**********\n",
      "Loss: 0.543700\n",
      "epoch: 132\n",
      "**********\n",
      "Loss: 0.534988\n",
      "epoch: 133\n",
      "**********\n",
      "Loss: 0.526557\n",
      "epoch: 134\n",
      "**********\n",
      "Loss: 0.518405\n",
      "epoch: 135\n",
      "**********\n",
      "Loss: 0.510508\n",
      "epoch: 136\n",
      "**********\n",
      "Loss: 0.502847\n",
      "epoch: 137\n",
      "**********\n",
      "Loss: 0.495417\n",
      "epoch: 138\n",
      "**********\n",
      "Loss: 0.488222\n",
      "epoch: 139\n",
      "**********\n",
      "Loss: 0.481227\n",
      "epoch: 140\n",
      "**********\n",
      "Loss: 0.474437\n",
      "epoch: 141\n",
      "**********\n",
      "Loss: 0.467841\n",
      "epoch: 142\n",
      "**********\n",
      "Loss: 0.461432\n",
      "epoch: 143\n",
      "**********\n",
      "Loss: 0.455193\n",
      "epoch: 144\n",
      "**********\n",
      "Loss: 0.449126\n",
      "epoch: 145\n",
      "**********\n",
      "Loss: 0.443215\n",
      "epoch: 146\n",
      "**********\n",
      "Loss: 0.437459\n",
      "epoch: 147\n",
      "**********\n",
      "Loss: 0.431844\n",
      "epoch: 148\n",
      "**********\n",
      "Loss: 0.426378\n",
      "epoch: 149\n",
      "**********\n",
      "Loss: 0.421039\n",
      "epoch: 150\n",
      "**********\n",
      "Loss: 0.415829\n",
      "epoch: 151\n",
      "**********\n",
      "Loss: 0.410742\n",
      "epoch: 152\n",
      "**********\n",
      "Loss: 0.405774\n",
      "epoch: 153\n",
      "**********\n",
      "Loss: 0.400912\n",
      "epoch: 154\n",
      "**********\n",
      "Loss: 0.396165\n",
      "epoch: 155\n",
      "**********\n",
      "Loss: 0.391512\n",
      "epoch: 156\n",
      "**********\n",
      "Loss: 0.386967\n",
      "epoch: 157\n",
      "**********\n",
      "Loss: 0.382510\n",
      "epoch: 158\n",
      "**********\n",
      "Loss: 0.378138\n",
      "epoch: 159\n",
      "**********\n",
      "Loss: 0.373865\n",
      "epoch: 160\n",
      "**********\n",
      "Loss: 0.369663\n",
      "epoch: 161\n",
      "**********\n",
      "Loss: 0.365550\n",
      "epoch: 162\n",
      "**********\n",
      "Loss: 0.361509\n",
      "epoch: 163\n",
      "**********\n",
      "Loss: 0.357537\n",
      "epoch: 164\n",
      "**********\n",
      "Loss: 0.353647\n",
      "epoch: 165\n",
      "**********\n",
      "Loss: 0.349816\n",
      "epoch: 166\n",
      "**********\n",
      "Loss: 0.346049\n",
      "epoch: 167\n",
      "**********\n",
      "Loss: 0.342354\n",
      "epoch: 168\n",
      "**********\n",
      "Loss: 0.338715\n",
      "epoch: 169\n",
      "**********\n",
      "Loss: 0.335132\n",
      "epoch: 170\n",
      "**********\n",
      "Loss: 0.331606\n",
      "epoch: 171\n",
      "**********\n",
      "Loss: 0.328138\n",
      "epoch: 172\n",
      "**********\n",
      "Loss: 0.324721\n",
      "epoch: 173\n",
      "**********\n",
      "Loss: 0.321354\n",
      "epoch: 174\n",
      "**********\n",
      "Loss: 0.318038\n",
      "epoch: 175\n",
      "**********\n",
      "Loss: 0.314766\n",
      "epoch: 176\n",
      "**********\n",
      "Loss: 0.311545\n",
      "epoch: 177\n",
      "**********\n",
      "Loss: 0.308368\n",
      "epoch: 178\n",
      "**********\n",
      "Loss: 0.305232\n",
      "epoch: 179\n",
      "**********\n",
      "Loss: 0.302139\n",
      "epoch: 180\n",
      "**********\n",
      "Loss: 0.299089\n",
      "epoch: 181\n",
      "**********\n",
      "Loss: 0.296088\n",
      "epoch: 182\n",
      "**********\n",
      "Loss: 0.293108\n",
      "epoch: 183\n",
      "**********\n",
      "Loss: 0.290174\n",
      "epoch: 184\n",
      "**********\n",
      "Loss: 0.287280\n",
      "epoch: 185\n",
      "**********\n",
      "Loss: 0.284416\n",
      "epoch: 186\n",
      "**********\n",
      "Loss: 0.281593\n",
      "epoch: 187\n",
      "**********\n",
      "Loss: 0.278795\n",
      "epoch: 188\n",
      "**********\n",
      "Loss: 0.276046\n",
      "epoch: 189\n",
      "**********\n",
      "Loss: 0.273323\n",
      "epoch: 190\n",
      "**********\n",
      "Loss: 0.270628\n",
      "epoch: 191\n",
      "**********\n",
      "Loss: 0.267969\n",
      "epoch: 192\n",
      "**********\n",
      "Loss: 0.265339\n",
      "epoch: 193\n",
      "**********\n",
      "Loss: 0.262740\n",
      "epoch: 194\n",
      "**********\n",
      "Loss: 0.260176\n",
      "epoch: 195\n",
      "**********\n",
      "Loss: 0.257634\n",
      "epoch: 196\n",
      "**********\n",
      "Loss: 0.255131\n",
      "epoch: 197\n",
      "**********\n",
      "Loss: 0.252644\n",
      "epoch: 198\n",
      "**********\n",
      "Loss: 0.250190\n",
      "epoch: 199\n",
      "**********\n",
      "Loss: 0.247771\n",
      "epoch: 200\n",
      "**********\n",
      "Loss: 0.245372\n",
      "epoch: 201\n",
      "**********\n",
      "Loss: 0.243003\n",
      "epoch: 202\n",
      "**********\n",
      "Loss: 0.240659\n",
      "epoch: 203\n",
      "**********\n",
      "Loss: 0.238333\n",
      "epoch: 204\n",
      "**********\n",
      "Loss: 0.236052\n",
      "epoch: 205\n",
      "**********\n",
      "Loss: 0.233779\n",
      "epoch: 206\n",
      "**********\n",
      "Loss: 0.231540\n",
      "epoch: 207\n",
      "**********\n",
      "Loss: 0.229316\n",
      "epoch: 208\n",
      "**********\n",
      "Loss: 0.227135\n",
      "epoch: 209\n",
      "**********\n",
      "Loss: 0.224963\n",
      "epoch: 210\n",
      "**********\n",
      "Loss: 0.222815\n",
      "epoch: 211\n",
      "**********\n",
      "Loss: 0.220692\n",
      "epoch: 212\n",
      "**********\n",
      "Loss: 0.218601\n",
      "epoch: 213\n",
      "**********\n",
      "Loss: 0.216518\n",
      "epoch: 214\n",
      "**********\n",
      "Loss: 0.214469\n",
      "epoch: 215\n",
      "**********\n",
      "Loss: 0.212432\n",
      "epoch: 216\n",
      "**********\n",
      "Loss: 0.210430\n",
      "epoch: 217\n",
      "**********\n",
      "Loss: 0.208434\n",
      "epoch: 218\n",
      "**********\n",
      "Loss: 0.206473\n",
      "epoch: 219\n",
      "**********\n",
      "Loss: 0.204527\n",
      "epoch: 220\n",
      "**********\n",
      "Loss: 0.202596\n",
      "epoch: 221\n",
      "**********\n",
      "Loss: 0.200694\n",
      "epoch: 222\n",
      "**********\n",
      "Loss: 0.198803\n",
      "epoch: 223\n",
      "**********\n",
      "Loss: 0.196951\n",
      "epoch: 224\n",
      "**********\n",
      "Loss: 0.195091\n",
      "epoch: 225\n",
      "**********\n",
      "Loss: 0.193267\n",
      "epoch: 226\n",
      "**********\n",
      "Loss: 0.191466\n",
      "epoch: 227\n",
      "**********\n",
      "Loss: 0.189668\n",
      "epoch: 228\n",
      "**********\n",
      "Loss: 0.187899\n",
      "epoch: 229\n",
      "**********\n",
      "Loss: 0.186151\n",
      "epoch: 230\n",
      "**********\n",
      "Loss: 0.184407\n",
      "epoch: 231\n",
      "**********\n",
      "Loss: 0.182694\n",
      "epoch: 232\n",
      "**********\n",
      "Loss: 0.180992\n",
      "epoch: 233\n",
      "**********\n",
      "Loss: 0.179310\n",
      "epoch: 234\n",
      "**********\n",
      "Loss: 0.177644\n",
      "epoch: 235\n",
      "**********\n",
      "Loss: 0.175992\n",
      "epoch: 236\n",
      "**********\n",
      "Loss: 0.174358\n",
      "epoch: 237\n",
      "**********\n",
      "Loss: 0.172747\n",
      "epoch: 238\n",
      "**********\n",
      "Loss: 0.171144\n",
      "epoch: 239\n",
      "**********\n",
      "Loss: 0.169556\n",
      "epoch: 240\n",
      "**********\n",
      "Loss: 0.167991\n",
      "epoch: 241\n",
      "**********\n",
      "Loss: 0.166438\n",
      "epoch: 242\n",
      "**********\n",
      "Loss: 0.164900\n",
      "epoch: 243\n",
      "**********\n",
      "Loss: 0.163377\n",
      "epoch: 244\n",
      "**********\n",
      "Loss: 0.161872\n",
      "epoch: 245\n",
      "**********\n",
      "Loss: 0.160377\n",
      "epoch: 246\n",
      "**********\n",
      "Loss: 0.158899\n",
      "epoch: 247\n",
      "**********\n",
      "Loss: 0.157440\n",
      "epoch: 248\n",
      "**********\n",
      "Loss: 0.155990\n",
      "epoch: 249\n",
      "**********\n",
      "Loss: 0.154556\n",
      "epoch: 250\n",
      "**********\n",
      "Loss: 0.153132\n",
      "epoch: 251\n",
      "**********\n",
      "Loss: 0.151731\n",
      "epoch: 252\n",
      "**********\n",
      "Loss: 0.150341\n",
      "epoch: 253\n",
      "**********\n",
      "Loss: 0.148958\n",
      "epoch: 254\n",
      "**********\n",
      "Loss: 0.147598\n",
      "epoch: 255\n",
      "**********\n",
      "Loss: 0.146246\n",
      "epoch: 256\n",
      "**********\n",
      "Loss: 0.144904\n",
      "epoch: 257\n",
      "**********\n",
      "Loss: 0.143586\n",
      "epoch: 258\n",
      "**********\n",
      "Loss: 0.142277\n",
      "epoch: 259\n",
      "**********\n",
      "Loss: 0.140974\n",
      "epoch: 260\n",
      "**********\n",
      "Loss: 0.139691\n",
      "epoch: 261\n",
      "**********\n",
      "Loss: 0.138419\n",
      "epoch: 262\n",
      "**********\n",
      "Loss: 0.137162\n",
      "epoch: 263\n",
      "**********\n",
      "Loss: 0.135916\n",
      "epoch: 264\n",
      "**********\n",
      "Loss: 0.134678\n",
      "epoch: 265\n",
      "**********\n",
      "Loss: 0.133462\n",
      "epoch: 266\n",
      "**********\n",
      "Loss: 0.132250\n",
      "epoch: 267\n",
      "**********\n",
      "Loss: 0.131051\n",
      "epoch: 268\n",
      "**********\n",
      "Loss: 0.129869\n",
      "epoch: 269\n",
      "**********\n",
      "Loss: 0.128699\n",
      "epoch: 270\n",
      "**********\n",
      "Loss: 0.127534\n",
      "epoch: 271\n",
      "**********\n",
      "Loss: 0.126388\n",
      "epoch: 272\n",
      "**********\n",
      "Loss: 0.125245\n",
      "epoch: 273\n",
      "**********\n",
      "Loss: 0.124126\n",
      "epoch: 274\n",
      "**********\n",
      "Loss: 0.123011\n",
      "epoch: 275\n",
      "**********\n",
      "Loss: 0.121908\n",
      "epoch: 276\n",
      "**********\n",
      "Loss: 0.120816\n",
      "epoch: 277\n",
      "**********\n",
      "Loss: 0.119737\n",
      "epoch: 278\n",
      "**********\n",
      "Loss: 0.118671\n",
      "epoch: 279\n",
      "**********\n",
      "Loss: 0.117611\n",
      "epoch: 280\n",
      "**********\n",
      "Loss: 0.116567\n",
      "epoch: 281\n",
      "**********\n",
      "Loss: 0.115530\n",
      "epoch: 282\n",
      "**********\n",
      "Loss: 0.114505\n",
      "epoch: 283\n",
      "**********\n",
      "Loss: 0.113496\n",
      "epoch: 284\n",
      "**********\n",
      "Loss: 0.112489\n",
      "epoch: 285\n",
      "**********\n",
      "Loss: 0.111499\n",
      "epoch: 286\n",
      "**********\n",
      "Loss: 0.110518\n",
      "epoch: 287\n",
      "**********\n",
      "Loss: 0.109547\n",
      "epoch: 288\n",
      "**********\n",
      "Loss: 0.108587\n",
      "epoch: 289\n",
      "**********\n",
      "Loss: 0.107636\n",
      "epoch: 290\n",
      "**********\n",
      "Loss: 0.106695\n",
      "epoch: 291\n",
      "**********\n",
      "Loss: 0.105765\n",
      "epoch: 292\n",
      "**********\n",
      "Loss: 0.104847\n",
      "epoch: 293\n",
      "**********\n",
      "Loss: 0.103936\n",
      "epoch: 294\n",
      "**********\n",
      "Loss: 0.103036\n",
      "epoch: 295\n",
      "**********\n",
      "Loss: 0.102145\n",
      "epoch: 296\n",
      "**********\n",
      "Loss: 0.101264\n",
      "epoch: 297\n",
      "**********\n",
      "Loss: 0.100395\n",
      "epoch: 298\n",
      "**********\n",
      "Loss: 0.099532\n",
      "epoch: 299\n",
      "**********\n",
      "Loss: 0.098679\n",
      "epoch: 300\n",
      "**********\n",
      "Loss: 0.097836\n",
      "epoch: 301\n",
      "**********\n",
      "Loss: 0.097003\n",
      "epoch: 302\n",
      "**********\n",
      "Loss: 0.096178\n",
      "epoch: 303\n",
      "**********\n",
      "Loss: 0.095363\n",
      "epoch: 304\n",
      "**********\n",
      "Loss: 0.094555\n",
      "epoch: 305\n",
      "**********\n",
      "Loss: 0.093760\n",
      "epoch: 306\n",
      "**********\n",
      "Loss: 0.092972\n",
      "epoch: 307\n",
      "**********\n",
      "Loss: 0.092191\n",
      "epoch: 308\n",
      "**********\n",
      "Loss: 0.091418\n",
      "epoch: 309\n",
      "**********\n",
      "Loss: 0.090659\n",
      "epoch: 310\n",
      "**********\n",
      "Loss: 0.089904\n",
      "epoch: 311\n",
      "**********\n",
      "Loss: 0.089159\n",
      "epoch: 312\n",
      "**********\n",
      "Loss: 0.088422\n",
      "epoch: 313\n",
      "**********\n",
      "Loss: 0.087691\n",
      "epoch: 314\n",
      "**********\n",
      "Loss: 0.086970\n",
      "epoch: 315\n",
      "**********\n",
      "Loss: 0.086260\n",
      "epoch: 316\n",
      "**********\n",
      "Loss: 0.085553\n",
      "epoch: 317\n",
      "**********\n",
      "Loss: 0.084859\n",
      "epoch: 318\n",
      "**********\n",
      "Loss: 0.084168\n",
      "epoch: 319\n",
      "**********\n",
      "Loss: 0.083485\n",
      "epoch: 320\n",
      "**********\n",
      "Loss: 0.082815\n",
      "epoch: 321\n",
      "**********\n",
      "Loss: 0.082148\n",
      "epoch: 322\n",
      "**********\n",
      "Loss: 0.081490\n",
      "epoch: 323\n",
      "**********\n",
      "Loss: 0.080840\n",
      "epoch: 324\n",
      "**********\n",
      "Loss: 0.080194\n",
      "epoch: 325\n",
      "**********\n",
      "Loss: 0.079560\n",
      "epoch: 326\n",
      "**********\n",
      "Loss: 0.078931\n",
      "epoch: 327\n",
      "**********\n",
      "Loss: 0.078307\n",
      "epoch: 328\n",
      "**********\n",
      "Loss: 0.077695\n",
      "epoch: 329\n",
      "**********\n",
      "Loss: 0.077086\n",
      "epoch: 330\n",
      "**********\n",
      "Loss: 0.076484\n",
      "epoch: 331\n",
      "**********\n",
      "Loss: 0.075893\n",
      "epoch: 332\n",
      "**********\n",
      "Loss: 0.075305\n",
      "epoch: 333\n",
      "**********\n",
      "Loss: 0.074723\n",
      "epoch: 334\n",
      "**********\n",
      "Loss: 0.074151\n",
      "epoch: 335\n",
      "**********\n",
      "Loss: 0.073583\n",
      "epoch: 336\n",
      "**********\n",
      "Loss: 0.073022\n",
      "epoch: 337\n",
      "**********\n",
      "Loss: 0.072469\n",
      "epoch: 338\n",
      "**********\n",
      "Loss: 0.071920\n",
      "epoch: 339\n",
      "**********\n",
      "Loss: 0.071378\n",
      "epoch: 340\n",
      "**********\n",
      "Loss: 0.070843\n",
      "epoch: 341\n",
      "**********\n",
      "Loss: 0.070314\n",
      "epoch: 342\n",
      "**********\n",
      "Loss: 0.069790\n",
      "epoch: 343\n",
      "**********\n",
      "Loss: 0.069273\n",
      "epoch: 344\n",
      "**********\n",
      "Loss: 0.068760\n",
      "epoch: 345\n",
      "**********\n",
      "Loss: 0.068256\n",
      "epoch: 346\n",
      "**********\n",
      "Loss: 0.067755\n",
      "epoch: 347\n",
      "**********\n",
      "Loss: 0.067261\n",
      "epoch: 348\n",
      "**********\n",
      "Loss: 0.066772\n",
      "epoch: 349\n",
      "**********\n",
      "Loss: 0.066289\n",
      "epoch: 350\n",
      "**********\n",
      "Loss: 0.065811\n",
      "epoch: 351\n",
      "**********\n",
      "Loss: 0.065340\n",
      "epoch: 352\n",
      "**********\n",
      "Loss: 0.064873\n",
      "epoch: 353\n",
      "**********\n",
      "Loss: 0.064410\n",
      "epoch: 354\n",
      "**********\n",
      "Loss: 0.063953\n",
      "epoch: 355\n",
      "**********\n",
      "Loss: 0.063503\n",
      "epoch: 356\n",
      "**********\n",
      "Loss: 0.063056\n",
      "epoch: 357\n",
      "**********\n",
      "Loss: 0.062616\n",
      "epoch: 358\n",
      "**********\n",
      "Loss: 0.062179\n",
      "epoch: 359\n",
      "**********\n",
      "Loss: 0.061747\n",
      "epoch: 360\n",
      "**********\n",
      "Loss: 0.061322\n",
      "epoch: 361\n",
      "**********\n",
      "Loss: 0.060900\n",
      "epoch: 362\n",
      "**********\n",
      "Loss: 0.060482\n",
      "epoch: 363\n",
      "**********\n",
      "Loss: 0.060070\n",
      "epoch: 364\n",
      "**********\n",
      "Loss: 0.059662\n",
      "epoch: 365\n",
      "**********\n",
      "Loss: 0.059259\n",
      "epoch: 366\n",
      "**********\n",
      "Loss: 0.058860\n",
      "epoch: 367\n",
      "**********\n",
      "Loss: 0.058466\n",
      "epoch: 368\n",
      "**********\n",
      "Loss: 0.058075\n",
      "epoch: 369\n",
      "**********\n",
      "Loss: 0.057690\n",
      "epoch: 370\n",
      "**********\n",
      "Loss: 0.057310\n",
      "epoch: 371\n",
      "**********\n",
      "Loss: 0.056931\n",
      "epoch: 372\n",
      "**********\n",
      "Loss: 0.056559\n",
      "epoch: 373\n",
      "**********\n",
      "Loss: 0.056190\n",
      "epoch: 374\n",
      "**********\n",
      "Loss: 0.055825\n",
      "epoch: 375\n",
      "**********\n",
      "Loss: 0.055464\n",
      "epoch: 376\n",
      "**********\n",
      "Loss: 0.055107\n",
      "epoch: 377\n",
      "**********\n",
      "Loss: 0.054755\n",
      "epoch: 378\n",
      "**********\n",
      "Loss: 0.054405\n",
      "epoch: 379\n",
      "**********\n",
      "Loss: 0.054060\n",
      "epoch: 380\n",
      "**********\n",
      "Loss: 0.053718\n",
      "epoch: 381\n",
      "**********\n",
      "Loss: 0.053380\n",
      "epoch: 382\n",
      "**********\n",
      "Loss: 0.053046\n",
      "epoch: 383\n",
      "**********\n",
      "Loss: 0.052715\n",
      "epoch: 384\n",
      "**********\n",
      "Loss: 0.052388\n",
      "epoch: 385\n",
      "**********\n",
      "Loss: 0.052064\n",
      "epoch: 386\n",
      "**********\n",
      "Loss: 0.051744\n",
      "epoch: 387\n",
      "**********\n",
      "Loss: 0.051427\n",
      "epoch: 388\n",
      "**********\n",
      "Loss: 0.051115\n",
      "epoch: 389\n",
      "**********\n",
      "Loss: 0.050804\n",
      "epoch: 390\n",
      "**********\n",
      "Loss: 0.050497\n",
      "epoch: 391\n",
      "**********\n",
      "Loss: 0.050194\n",
      "epoch: 392\n",
      "**********\n",
      "Loss: 0.049893\n",
      "epoch: 393\n",
      "**********\n",
      "Loss: 0.049596\n",
      "epoch: 394\n",
      "**********\n",
      "Loss: 0.049302\n",
      "epoch: 395\n",
      "**********\n",
      "Loss: 0.049011\n",
      "epoch: 396\n",
      "**********\n",
      "Loss: 0.048723\n",
      "epoch: 397\n",
      "**********\n",
      "Loss: 0.048439\n",
      "epoch: 398\n",
      "**********\n",
      "Loss: 0.048156\n",
      "epoch: 399\n",
      "**********\n",
      "Loss: 0.047877\n",
      "epoch: 400\n",
      "**********\n",
      "Loss: 0.047601\n",
      "epoch: 401\n",
      "**********\n",
      "Loss: 0.047328\n",
      "epoch: 402\n",
      "**********\n",
      "Loss: 0.047057\n",
      "epoch: 403\n",
      "**********\n",
      "Loss: 0.046790\n",
      "epoch: 404\n",
      "**********\n",
      "Loss: 0.046525\n",
      "epoch: 405\n",
      "**********\n",
      "Loss: 0.046262\n",
      "epoch: 406\n",
      "**********\n",
      "Loss: 0.046003\n",
      "epoch: 407\n",
      "**********\n",
      "Loss: 0.045747\n",
      "epoch: 408\n",
      "**********\n",
      "Loss: 0.045492\n",
      "epoch: 409\n",
      "**********\n",
      "Loss: 0.045241\n",
      "epoch: 410\n",
      "**********\n",
      "Loss: 0.044991\n",
      "epoch: 411\n",
      "**********\n",
      "Loss: 0.044745\n",
      "epoch: 412\n",
      "**********\n",
      "Loss: 0.044501\n",
      "epoch: 413\n",
      "**********\n",
      "Loss: 0.044259\n",
      "epoch: 414\n",
      "**********\n",
      "Loss: 0.044020\n",
      "epoch: 415\n",
      "**********\n",
      "Loss: 0.043783\n",
      "epoch: 416\n",
      "**********\n",
      "Loss: 0.043549\n",
      "epoch: 417\n",
      "**********\n",
      "Loss: 0.043316\n",
      "epoch: 418\n",
      "**********\n",
      "Loss: 0.043087\n",
      "epoch: 419\n",
      "**********\n",
      "Loss: 0.042859\n",
      "epoch: 420\n",
      "**********\n",
      "Loss: 0.042634\n",
      "epoch: 421\n",
      "**********\n",
      "Loss: 0.042411\n",
      "epoch: 422\n",
      "**********\n",
      "Loss: 0.042190\n",
      "epoch: 423\n",
      "**********\n",
      "Loss: 0.041972\n",
      "epoch: 424\n",
      "**********\n",
      "Loss: 0.041755\n",
      "epoch: 425\n",
      "**********\n",
      "Loss: 0.041541\n",
      "epoch: 426\n",
      "**********\n",
      "Loss: 0.041328\n",
      "epoch: 427\n",
      "**********\n",
      "Loss: 0.041118\n",
      "epoch: 428\n",
      "**********\n",
      "Loss: 0.040910\n",
      "epoch: 429\n",
      "**********\n",
      "Loss: 0.040704\n",
      "epoch: 430\n",
      "**********\n",
      "Loss: 0.040499\n",
      "epoch: 431\n",
      "**********\n",
      "Loss: 0.040297\n",
      "epoch: 432\n",
      "**********\n",
      "Loss: 0.040097\n",
      "epoch: 433\n",
      "**********\n",
      "Loss: 0.039898\n",
      "epoch: 434\n",
      "**********\n",
      "Loss: 0.039702\n",
      "epoch: 435\n",
      "**********\n",
      "Loss: 0.039507\n",
      "epoch: 436\n",
      "**********\n",
      "Loss: 0.039314\n",
      "epoch: 437\n",
      "**********\n",
      "Loss: 0.039123\n",
      "epoch: 438\n",
      "**********\n",
      "Loss: 0.038935\n",
      "epoch: 439\n",
      "**********\n",
      "Loss: 0.038747\n",
      "epoch: 440\n",
      "**********\n",
      "Loss: 0.038561\n",
      "epoch: 441\n",
      "**********\n",
      "Loss: 0.038377\n",
      "epoch: 442\n",
      "**********\n",
      "Loss: 0.038195\n",
      "epoch: 443\n",
      "**********\n",
      "Loss: 0.038015\n",
      "epoch: 444\n",
      "**********\n",
      "Loss: 0.037836\n",
      "epoch: 445\n",
      "**********\n",
      "Loss: 0.037659\n",
      "epoch: 446\n",
      "**********\n",
      "Loss: 0.037484\n",
      "epoch: 447\n",
      "**********\n",
      "Loss: 0.037310\n",
      "epoch: 448\n",
      "**********\n",
      "Loss: 0.037138\n",
      "epoch: 449\n",
      "**********\n",
      "Loss: 0.036967\n",
      "epoch: 450\n",
      "**********\n",
      "Loss: 0.036798\n",
      "epoch: 451\n",
      "**********\n",
      "Loss: 0.036631\n",
      "epoch: 452\n",
      "**********\n",
      "Loss: 0.036464\n",
      "epoch: 453\n",
      "**********\n",
      "Loss: 0.036300\n",
      "epoch: 454\n",
      "**********\n",
      "Loss: 0.036137\n",
      "epoch: 455\n",
      "**********\n",
      "Loss: 0.035975\n",
      "epoch: 456\n",
      "**********\n",
      "Loss: 0.035815\n",
      "epoch: 457\n",
      "**********\n",
      "Loss: 0.035657\n",
      "epoch: 458\n",
      "**********\n",
      "Loss: 0.035500\n",
      "epoch: 459\n",
      "**********\n",
      "Loss: 0.035344\n",
      "epoch: 460\n",
      "**********\n",
      "Loss: 0.035190\n",
      "epoch: 461\n",
      "**********\n",
      "Loss: 0.035036\n",
      "epoch: 462\n",
      "**********\n",
      "Loss: 0.034885\n",
      "epoch: 463\n",
      "**********\n",
      "Loss: 0.034735\n",
      "epoch: 464\n",
      "**********\n",
      "Loss: 0.034586\n",
      "epoch: 465\n",
      "**********\n",
      "Loss: 0.034438\n",
      "epoch: 466\n",
      "**********\n",
      "Loss: 0.034292\n",
      "epoch: 467\n",
      "**********\n",
      "Loss: 0.034147\n",
      "epoch: 468\n",
      "**********\n",
      "Loss: 0.034003\n",
      "epoch: 469\n",
      "**********\n",
      "Loss: 0.033861\n",
      "epoch: 470\n",
      "**********\n",
      "Loss: 0.033720\n",
      "epoch: 471\n",
      "**********\n",
      "Loss: 0.033580\n",
      "epoch: 472\n",
      "**********\n",
      "Loss: 0.033441\n",
      "epoch: 473\n",
      "**********\n",
      "Loss: 0.033304\n",
      "epoch: 474\n",
      "**********\n",
      "Loss: 0.033167\n",
      "epoch: 475\n",
      "**********\n",
      "Loss: 0.033032\n",
      "epoch: 476\n",
      "**********\n",
      "Loss: 0.032898\n",
      "epoch: 477\n",
      "**********\n",
      "Loss: 0.032765\n",
      "epoch: 478\n",
      "**********\n",
      "Loss: 0.032633\n",
      "epoch: 479\n",
      "**********\n",
      "Loss: 0.032502\n",
      "epoch: 480\n",
      "**********\n",
      "Loss: 0.032372\n",
      "epoch: 481\n",
      "**********\n",
      "Loss: 0.032244\n",
      "epoch: 482\n",
      "**********\n",
      "Loss: 0.032117\n",
      "epoch: 483\n",
      "**********\n",
      "Loss: 0.031990\n",
      "epoch: 484\n",
      "**********\n",
      "Loss: 0.031865\n",
      "epoch: 485\n",
      "**********\n",
      "Loss: 0.031741\n",
      "epoch: 486\n",
      "**********\n",
      "Loss: 0.031618\n",
      "epoch: 487\n",
      "**********\n",
      "Loss: 0.031496\n",
      "epoch: 488\n",
      "**********\n",
      "Loss: 0.031375\n",
      "epoch: 489\n",
      "**********\n",
      "Loss: 0.031255\n",
      "epoch: 490\n",
      "**********\n",
      "Loss: 0.031136\n",
      "epoch: 491\n",
      "**********\n",
      "Loss: 0.031018\n",
      "epoch: 492\n",
      "**********\n",
      "Loss: 0.030900\n",
      "epoch: 493\n",
      "**********\n",
      "Loss: 0.030784\n",
      "epoch: 494\n",
      "**********\n",
      "Loss: 0.030669\n",
      "epoch: 495\n",
      "**********\n",
      "Loss: 0.030555\n",
      "epoch: 496\n",
      "**********\n",
      "Loss: 0.030441\n",
      "epoch: 497\n",
      "**********\n",
      "Loss: 0.030329\n",
      "epoch: 498\n",
      "**********\n",
      "Loss: 0.030217\n",
      "epoch: 499\n",
      "**********\n",
      "Loss: 0.030107\n",
      "epoch: 500\n",
      "**********\n",
      "Loss: 0.029997\n"
     ]
    }
   ],
   "source": [
    "train(onelayertransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2221a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['shall', 'besiege', 'thy', 'brow,', 'And'], predicted words are ['Proving', 'Proving', 'lies,', 'being', 'fair']\n"
     ]
    }
   ],
   "source": [
    "evaluate(onelayertransformer, 2, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "45852383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['brow,', 'And', 'dig', 'deep', 'trenches'], predicted words are ['being', 'fair', 'art', 'eyes,', 'Thy']\n"
     ]
    }
   ],
   "source": [
    "evaluate(onelayertransformer, 5, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6090b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty', 'winters', 'shall', 'besiege'],\n",
       "  ['forty', 'winters', 'shall', 'besiege', 'thy']),\n",
       " (['forty', 'winters', 'shall', 'besiege', 'thy'],\n",
       "  ['winters', 'shall', 'besiege', 'thy', 'brow,']),\n",
       " (['winters', 'shall', 'besiege', 'thy', 'brow,'],\n",
       "  ['shall', 'besiege', 'thy', 'brow,', 'And']),\n",
       " (['shall', 'besiege', 'thy', 'brow,', 'And'],\n",
       "  ['besiege', 'thy', 'brow,', 'And', 'dig']),\n",
       " (['besiege', 'thy', 'brow,', 'And', 'dig'],\n",
       "  ['thy', 'brow,', 'And', 'dig', 'deep']),\n",
       " (['thy', 'brow,', 'And', 'dig', 'deep'],\n",
       "  ['brow,', 'And', 'dig', 'deep', 'trenches']),\n",
       " (['brow,', 'And', 'dig', 'deep', 'trenches'],\n",
       "  ['And', 'dig', 'deep', 'trenches', 'in']),\n",
       " (['And', 'dig', 'deep', 'trenches', 'in'],\n",
       "  ['dig', 'deep', 'trenches', 'in', 'thy']),\n",
       " (['dig', 'deep', 'trenches', 'in', 'thy'],\n",
       "  ['deep', 'trenches', 'in', 'thy', \"beauty's\"]),\n",
       " (['deep', 'trenches', 'in', 'thy', \"beauty's\"],\n",
       "  ['trenches', 'in', 'thy', \"beauty's\", 'field,'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d8734",
   "metadata": {},
   "source": [
    "## TwoLayerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "70f42069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size: int, heads_num: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(hidden_size=hidden_size, heads_num=heads_num)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x0, attention_score = self.mha(x, x, x)\n",
    "        x1 = ## Fill this line # residual connection \n",
    "        \n",
    "        return x1, attention_score\n",
    "        \n",
    "class TwoLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, token_embed_size: int, hidden_size: int, heads_num: int):\n",
    "        super(TwoLayerTransformer, self).__init__()\n",
    "        self.n_word = vocab_size\n",
    "        self.token_embedding = nn.Embedding(self.n_word, token_embed_size)\n",
    "        self.embed = ## Fill this line # residual connection\n",
    "        self.unembed = ## Fill this line # residual connection\n",
    "        \n",
    "        self.decoders = clones(Decoder(hidden_size=hidden_size, heads_num=heads_num), N=2) # 2 layers\n",
    "        \n",
    "    def forward(self, x: Tensor, with_attention: bool = False) -> Tensor:\n",
    "        token_embed = self.token_embedding(x)\n",
    "        embed = self.embed(token_embed)\n",
    "        x = F.relu(embed)\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            x, attention_score = decoder(x)\n",
    "            \n",
    "        output = self.unembed(x)\n",
    "        log_prob = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        if with_attention:\n",
    "            return log_prob, attention_score\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9898f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolayertransformer = TwoLayerTransformer(\n",
    "    vocab_size=len(word_to_idx),\n",
    "    token_embed_size=100,\n",
    "    hidden_size=128,\n",
    "    heads_num=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bc028b59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.243575\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.196825\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.150759\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.105265\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.060147\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 5.015346\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.970820\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.926529\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.882434\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.838496\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.794660\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.750861\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.707050\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.663205\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 4.619269\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 4.575270\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 4.531198\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 4.487051\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 4.442785\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 4.398390\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 4.353848\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 4.309173\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 4.264312\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 4.219270\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 4.174039\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 4.128508\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 4.082656\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 4.036506\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 3.989996\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 3.943090\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 3.895765\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 3.847982\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 3.799717\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 3.750941\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 3.701641\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 3.651827\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 3.601498\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 3.550656\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 3.499329\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 3.447508\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 3.395198\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 3.342482\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 3.289325\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 3.235697\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 3.181643\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 3.127187\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 3.072355\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 3.017162\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 2.961633\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 2.905767\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 2.849598\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 2.793180\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 2.736541\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 2.679678\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 2.622646\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 2.565471\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 2.508206\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 2.450884\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 2.393590\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 2.336313\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 2.279193\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 2.222212\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 2.165462\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 2.109013\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 2.052855\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 1.997142\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 1.941939\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 1.887269\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 1.833240\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 1.779851\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 1.727224\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 1.675401\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 1.624457\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 1.574431\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 1.525386\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 1.477356\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 1.430401\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 1.384575\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 1.339897\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 1.296363\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 1.254053\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 1.212953\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 1.173099\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 1.134508\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 1.097147\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 1.061060\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 1.026217\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.992636\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.960282\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.929159\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.899236\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.870486\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.842891\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.816426\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.791022\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.766684\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.743356\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.721019\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.699621\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.679116\n",
      "epoch: 101\n",
      "**********\n",
      "Loss: 0.659506\n",
      "epoch: 102\n",
      "**********\n",
      "Loss: 0.640705\n",
      "epoch: 103\n",
      "**********\n",
      "Loss: 0.622692\n",
      "epoch: 104\n",
      "**********\n",
      "Loss: 0.605432\n",
      "epoch: 105\n",
      "**********\n",
      "Loss: 0.588900\n",
      "epoch: 106\n",
      "**********\n",
      "Loss: 0.573060\n",
      "epoch: 107\n",
      "**********\n",
      "Loss: 0.557848\n",
      "epoch: 108\n",
      "**********\n",
      "Loss: 0.543279\n",
      "epoch: 109\n",
      "**********\n",
      "Loss: 0.529278\n",
      "epoch: 110\n",
      "**********\n",
      "Loss: 0.515850\n",
      "epoch: 111\n",
      "**********\n",
      "Loss: 0.502947\n",
      "epoch: 112\n",
      "**********\n",
      "Loss: 0.490547\n",
      "epoch: 113\n",
      "**********\n",
      "Loss: 0.478619\n",
      "epoch: 114\n",
      "**********\n",
      "Loss: 0.467154\n",
      "epoch: 115\n",
      "**********\n",
      "Loss: 0.456102\n",
      "epoch: 116\n",
      "**********\n",
      "Loss: 0.445472\n",
      "epoch: 117\n",
      "**********\n",
      "Loss: 0.435219\n",
      "epoch: 118\n",
      "**********\n",
      "Loss: 0.425334\n",
      "epoch: 119\n",
      "**********\n",
      "Loss: 0.415804\n",
      "epoch: 120\n",
      "**********\n",
      "Loss: 0.406596\n",
      "epoch: 121\n",
      "**********\n",
      "Loss: 0.397705\n",
      "epoch: 122\n",
      "**********\n",
      "Loss: 0.389117\n",
      "epoch: 123\n",
      "**********\n",
      "Loss: 0.380808\n",
      "epoch: 124\n",
      "**********\n",
      "Loss: 0.372776\n",
      "epoch: 125\n",
      "**********\n",
      "Loss: 0.365009\n",
      "epoch: 126\n",
      "**********\n",
      "Loss: 0.357477\n",
      "epoch: 127\n",
      "**********\n",
      "Loss: 0.350187\n",
      "epoch: 128\n",
      "**********\n",
      "Loss: 0.343122\n",
      "epoch: 129\n",
      "**********\n",
      "Loss: 0.336277\n",
      "epoch: 130\n",
      "**********\n",
      "Loss: 0.329630\n",
      "epoch: 131\n",
      "**********\n",
      "Loss: 0.323195\n",
      "epoch: 132\n",
      "**********\n",
      "Loss: 0.316937\n",
      "epoch: 133\n",
      "**********\n",
      "Loss: 0.310869\n",
      "epoch: 134\n",
      "**********\n",
      "Loss: 0.304972\n",
      "epoch: 135\n",
      "**********\n",
      "Loss: 0.299245\n",
      "epoch: 136\n",
      "**********\n",
      "Loss: 0.293679\n",
      "epoch: 137\n",
      "**********\n",
      "Loss: 0.288269\n",
      "epoch: 138\n",
      "**********\n",
      "Loss: 0.283006\n",
      "epoch: 139\n",
      "**********\n",
      "Loss: 0.277887\n",
      "epoch: 140\n",
      "**********\n",
      "Loss: 0.272909\n",
      "epoch: 141\n",
      "**********\n",
      "Loss: 0.268061\n",
      "epoch: 142\n",
      "**********\n",
      "Loss: 0.263342\n",
      "epoch: 143\n",
      "**********\n",
      "Loss: 0.258751\n",
      "epoch: 144\n",
      "**********\n",
      "Loss: 0.254282\n",
      "epoch: 145\n",
      "**********\n",
      "Loss: 0.249923\n",
      "epoch: 146\n",
      "**********\n",
      "Loss: 0.245677\n",
      "epoch: 147\n",
      "**********\n",
      "Loss: 0.241544\n",
      "epoch: 148\n",
      "**********\n",
      "Loss: 0.237508\n",
      "epoch: 149\n",
      "**********\n",
      "Loss: 0.233577\n",
      "epoch: 150\n",
      "**********\n",
      "Loss: 0.229743\n",
      "epoch: 151\n",
      "**********\n",
      "Loss: 0.226003\n",
      "epoch: 152\n",
      "**********\n",
      "Loss: 0.222353\n",
      "epoch: 153\n",
      "**********\n",
      "Loss: 0.218796\n",
      "epoch: 154\n",
      "**********\n",
      "Loss: 0.215320\n",
      "epoch: 155\n",
      "**********\n",
      "Loss: 0.211933\n",
      "epoch: 156\n",
      "**********\n",
      "Loss: 0.208618\n",
      "epoch: 157\n",
      "**********\n",
      "Loss: 0.205388\n",
      "epoch: 158\n",
      "**********\n",
      "Loss: 0.202232\n",
      "epoch: 159\n",
      "**********\n",
      "Loss: 0.199145\n",
      "epoch: 160\n",
      "**********\n",
      "Loss: 0.196141\n",
      "epoch: 161\n",
      "**********\n",
      "Loss: 0.193198\n",
      "epoch: 162\n",
      "**********\n",
      "Loss: 0.190324\n",
      "epoch: 163\n",
      "**********\n",
      "Loss: 0.187514\n",
      "epoch: 164\n",
      "**********\n",
      "Loss: 0.184768\n",
      "epoch: 165\n",
      "**********\n",
      "Loss: 0.182083\n",
      "epoch: 166\n",
      "**********\n",
      "Loss: 0.179458\n",
      "epoch: 167\n",
      "**********\n",
      "Loss: 0.176892\n",
      "epoch: 168\n",
      "**********\n",
      "Loss: 0.174380\n",
      "epoch: 169\n",
      "**********\n",
      "Loss: 0.171921\n",
      "epoch: 170\n",
      "**********\n",
      "Loss: 0.169519\n",
      "epoch: 171\n",
      "**********\n",
      "Loss: 0.167166\n",
      "epoch: 172\n",
      "**********\n",
      "Loss: 0.164862\n",
      "epoch: 173\n",
      "**********\n",
      "Loss: 0.162608\n",
      "epoch: 174\n",
      "**********\n",
      "Loss: 0.160403\n",
      "epoch: 175\n",
      "**********\n",
      "Loss: 0.158242\n",
      "epoch: 176\n",
      "**********\n",
      "Loss: 0.156125\n",
      "epoch: 177\n",
      "**********\n",
      "Loss: 0.154054\n",
      "epoch: 178\n",
      "**********\n",
      "Loss: 0.152024\n",
      "epoch: 179\n",
      "**********\n",
      "Loss: 0.150035\n",
      "epoch: 180\n",
      "**********\n",
      "Loss: 0.148089\n",
      "epoch: 181\n",
      "**********\n",
      "Loss: 0.146179\n",
      "epoch: 182\n",
      "**********\n",
      "Loss: 0.144308\n",
      "epoch: 183\n",
      "**********\n",
      "Loss: 0.142474\n",
      "epoch: 184\n",
      "**********\n",
      "Loss: 0.140677\n",
      "epoch: 185\n",
      "**********\n",
      "Loss: 0.138916\n",
      "epoch: 186\n",
      "**********\n",
      "Loss: 0.137189\n",
      "epoch: 187\n",
      "**********\n",
      "Loss: 0.135496\n",
      "epoch: 188\n",
      "**********\n",
      "Loss: 0.133834\n",
      "epoch: 189\n",
      "**********\n",
      "Loss: 0.132206\n",
      "epoch: 190\n",
      "**********\n",
      "Loss: 0.130609\n",
      "epoch: 191\n",
      "**********\n",
      "Loss: 0.129042\n",
      "epoch: 192\n",
      "**********\n",
      "Loss: 0.127506\n",
      "epoch: 193\n",
      "**********\n",
      "Loss: 0.125996\n",
      "epoch: 194\n",
      "**********\n",
      "Loss: 0.124516\n",
      "epoch: 195\n",
      "**********\n",
      "Loss: 0.123064\n",
      "epoch: 196\n",
      "**********\n",
      "Loss: 0.121640\n",
      "epoch: 197\n",
      "**********\n",
      "Loss: 0.120240\n",
      "epoch: 198\n",
      "**********\n",
      "Loss: 0.118866\n",
      "epoch: 199\n",
      "**********\n",
      "Loss: 0.117519\n",
      "epoch: 200\n",
      "**********\n",
      "Loss: 0.116193\n",
      "epoch: 201\n",
      "**********\n",
      "Loss: 0.114894\n",
      "epoch: 202\n",
      "**********\n",
      "Loss: 0.113616\n",
      "epoch: 203\n",
      "**********\n",
      "Loss: 0.112362\n",
      "epoch: 204\n",
      "**********\n",
      "Loss: 0.111132\n",
      "epoch: 205\n",
      "**********\n",
      "Loss: 0.109921\n",
      "epoch: 206\n",
      "**********\n",
      "Loss: 0.108733\n",
      "epoch: 207\n",
      "**********\n",
      "Loss: 0.107566\n",
      "epoch: 208\n",
      "**********\n",
      "Loss: 0.106419\n",
      "epoch: 209\n",
      "**********\n",
      "Loss: 0.105292\n",
      "epoch: 210\n",
      "**********\n",
      "Loss: 0.104184\n",
      "epoch: 211\n",
      "**********\n",
      "Loss: 0.103094\n",
      "epoch: 212\n",
      "**********\n",
      "Loss: 0.102024\n",
      "epoch: 213\n",
      "**********\n",
      "Loss: 0.100970\n",
      "epoch: 214\n",
      "**********\n",
      "Loss: 0.099938\n",
      "epoch: 215\n",
      "**********\n",
      "Loss: 0.098920\n",
      "epoch: 216\n",
      "**********\n",
      "Loss: 0.097920\n",
      "epoch: 217\n",
      "**********\n",
      "Loss: 0.096936\n",
      "epoch: 218\n",
      "**********\n",
      "Loss: 0.095969\n",
      "epoch: 219\n",
      "**********\n",
      "Loss: 0.095016\n",
      "epoch: 220\n",
      "**********\n",
      "Loss: 0.094082\n",
      "epoch: 221\n",
      "**********\n",
      "Loss: 0.093161\n",
      "epoch: 222\n",
      "**********\n",
      "Loss: 0.092257\n",
      "epoch: 223\n",
      "**********\n",
      "Loss: 0.091365\n",
      "epoch: 224\n",
      "**********\n",
      "Loss: 0.090489\n",
      "epoch: 225\n",
      "**********\n",
      "Loss: 0.089626\n",
      "epoch: 226\n",
      "**********\n",
      "Loss: 0.088778\n",
      "epoch: 227\n",
      "**********\n",
      "Loss: 0.087942\n",
      "epoch: 228\n",
      "**********\n",
      "Loss: 0.087120\n",
      "epoch: 229\n",
      "**********\n",
      "Loss: 0.086313\n",
      "epoch: 230\n",
      "**********\n",
      "Loss: 0.085516\n",
      "epoch: 231\n",
      "**********\n",
      "Loss: 0.084733\n",
      "epoch: 232\n",
      "**********\n",
      "Loss: 0.083961\n",
      "epoch: 233\n",
      "**********\n",
      "Loss: 0.083202\n",
      "epoch: 234\n",
      "**********\n",
      "Loss: 0.082454\n",
      "epoch: 235\n",
      "**********\n",
      "Loss: 0.081718\n",
      "epoch: 236\n",
      "**********\n",
      "Loss: 0.080993\n",
      "epoch: 237\n",
      "**********\n",
      "Loss: 0.080280\n",
      "epoch: 238\n",
      "**********\n",
      "Loss: 0.079576\n",
      "epoch: 239\n",
      "**********\n",
      "Loss: 0.078884\n",
      "epoch: 240\n",
      "**********\n",
      "Loss: 0.078203\n",
      "epoch: 241\n",
      "**********\n",
      "Loss: 0.077532\n",
      "epoch: 242\n",
      "**********\n",
      "Loss: 0.076870\n",
      "epoch: 243\n",
      "**********\n",
      "Loss: 0.076219\n",
      "epoch: 244\n",
      "**********\n",
      "Loss: 0.075578\n",
      "epoch: 245\n",
      "**********\n",
      "Loss: 0.074946\n",
      "epoch: 246\n",
      "**********\n",
      "Loss: 0.074324\n",
      "epoch: 247\n",
      "**********\n",
      "Loss: 0.073709\n",
      "epoch: 248\n",
      "**********\n",
      "Loss: 0.073106\n",
      "epoch: 249\n",
      "**********\n",
      "Loss: 0.072510\n",
      "epoch: 250\n",
      "**********\n",
      "Loss: 0.071923\n",
      "epoch: 251\n",
      "**********\n",
      "Loss: 0.071344\n",
      "epoch: 252\n",
      "**********\n",
      "Loss: 0.070775\n",
      "epoch: 253\n",
      "**********\n",
      "Loss: 0.070213\n",
      "epoch: 254\n",
      "**********\n",
      "Loss: 0.069660\n",
      "epoch: 255\n",
      "**********\n",
      "Loss: 0.069114\n",
      "epoch: 256\n",
      "**********\n",
      "Loss: 0.068576\n",
      "epoch: 257\n",
      "**********\n",
      "Loss: 0.068045\n",
      "epoch: 258\n",
      "**********\n",
      "Loss: 0.067523\n",
      "epoch: 259\n",
      "**********\n",
      "Loss: 0.067008\n",
      "epoch: 260\n",
      "**********\n",
      "Loss: 0.066496\n",
      "epoch: 261\n",
      "**********\n",
      "Loss: 0.065995\n",
      "epoch: 262\n",
      "**********\n",
      "Loss: 0.065502\n",
      "epoch: 263\n",
      "**********\n",
      "Loss: 0.065014\n",
      "epoch: 264\n",
      "**********\n",
      "Loss: 0.064534\n",
      "epoch: 265\n",
      "**********\n",
      "Loss: 0.064060\n",
      "epoch: 266\n",
      "**********\n",
      "Loss: 0.063593\n",
      "epoch: 267\n",
      "**********\n",
      "Loss: 0.063132\n",
      "epoch: 268\n",
      "**********\n",
      "Loss: 0.062677\n",
      "epoch: 269\n",
      "**********\n",
      "Loss: 0.062228\n",
      "epoch: 270\n",
      "**********\n",
      "Loss: 0.061786\n",
      "epoch: 271\n",
      "**********\n",
      "Loss: 0.061350\n",
      "epoch: 272\n",
      "**********\n",
      "Loss: 0.060918\n",
      "epoch: 273\n",
      "**********\n",
      "Loss: 0.060493\n",
      "epoch: 274\n",
      "**********\n",
      "Loss: 0.060073\n",
      "epoch: 275\n",
      "**********\n",
      "Loss: 0.059660\n",
      "epoch: 276\n",
      "**********\n",
      "Loss: 0.059251\n",
      "epoch: 277\n",
      "**********\n",
      "Loss: 0.058848\n",
      "epoch: 278\n",
      "**********\n",
      "Loss: 0.058450\n",
      "epoch: 279\n",
      "**********\n",
      "Loss: 0.058057\n",
      "epoch: 280\n",
      "**********\n",
      "Loss: 0.057669\n",
      "epoch: 281\n",
      "**********\n",
      "Loss: 0.057287\n",
      "epoch: 282\n",
      "**********\n",
      "Loss: 0.056909\n",
      "epoch: 283\n",
      "**********\n",
      "Loss: 0.056535\n",
      "epoch: 284\n",
      "**********\n",
      "Loss: 0.056171\n",
      "epoch: 285\n",
      "**********\n",
      "Loss: 0.055803\n",
      "epoch: 286\n",
      "**********\n",
      "Loss: 0.055449\n",
      "epoch: 287\n",
      "**********\n",
      "Loss: 0.055090\n",
      "epoch: 288\n",
      "**********\n",
      "Loss: 0.054740\n",
      "epoch: 289\n",
      "**********\n",
      "Loss: 0.054393\n",
      "epoch: 290\n",
      "**********\n",
      "Loss: 0.054056\n",
      "epoch: 291\n",
      "**********\n",
      "Loss: 0.053714\n",
      "epoch: 292\n",
      "**********\n",
      "Loss: 0.053385\n",
      "epoch: 293\n",
      "**********\n",
      "Loss: 0.053051\n",
      "epoch: 294\n",
      "**********\n",
      "Loss: 0.052730\n",
      "epoch: 295\n",
      "**********\n",
      "Loss: 0.052405\n",
      "epoch: 296\n",
      "**********\n",
      "Loss: 0.052088\n",
      "epoch: 297\n",
      "**********\n",
      "Loss: 0.051774\n",
      "epoch: 298\n",
      "**********\n",
      "Loss: 0.051464\n",
      "epoch: 299\n",
      "**********\n",
      "Loss: 0.051157\n",
      "epoch: 300\n",
      "**********\n",
      "Loss: 0.050855\n",
      "epoch: 301\n",
      "**********\n",
      "Loss: 0.050555\n",
      "epoch: 302\n",
      "**********\n",
      "Loss: 0.050260\n",
      "epoch: 303\n",
      "**********\n",
      "Loss: 0.049968\n",
      "epoch: 304\n",
      "**********\n",
      "Loss: 0.049679\n",
      "epoch: 305\n",
      "**********\n",
      "Loss: 0.049393\n",
      "epoch: 306\n",
      "**********\n",
      "Loss: 0.049111\n",
      "epoch: 307\n",
      "**********\n",
      "Loss: 0.048832\n",
      "epoch: 308\n",
      "**********\n",
      "Loss: 0.048556\n",
      "epoch: 309\n",
      "**********\n",
      "Loss: 0.048283\n",
      "epoch: 310\n",
      "**********\n",
      "Loss: 0.048013\n",
      "epoch: 311\n",
      "**********\n",
      "Loss: 0.047747\n",
      "epoch: 312\n",
      "**********\n",
      "Loss: 0.047483\n",
      "epoch: 313\n",
      "**********\n",
      "Loss: 0.047222\n",
      "epoch: 314\n",
      "**********\n",
      "Loss: 0.046964\n",
      "epoch: 315\n",
      "**********\n",
      "Loss: 0.046709\n",
      "epoch: 316\n",
      "**********\n",
      "Loss: 0.046457\n",
      "epoch: 317\n",
      "**********\n",
      "Loss: 0.046207\n",
      "epoch: 318\n",
      "**********\n",
      "Loss: 0.045961\n",
      "epoch: 319\n",
      "**********\n",
      "Loss: 0.045717\n",
      "epoch: 320\n",
      "**********\n",
      "Loss: 0.045475\n",
      "epoch: 321\n",
      "**********\n",
      "Loss: 0.045236\n",
      "epoch: 322\n",
      "**********\n",
      "Loss: 0.045000\n",
      "epoch: 323\n",
      "**********\n",
      "Loss: 0.044766\n",
      "epoch: 324\n",
      "**********\n",
      "Loss: 0.044535\n",
      "epoch: 325\n",
      "**********\n",
      "Loss: 0.044306\n",
      "epoch: 326\n",
      "**********\n",
      "Loss: 0.044079\n",
      "epoch: 327\n",
      "**********\n",
      "Loss: 0.043856\n",
      "epoch: 328\n",
      "**********\n",
      "Loss: 0.043633\n",
      "epoch: 329\n",
      "**********\n",
      "Loss: 0.043418\n",
      "epoch: 330\n",
      "**********\n",
      "Loss: 0.043197\n",
      "epoch: 331\n",
      "**********\n",
      "Loss: 0.042982\n",
      "epoch: 332\n",
      "**********\n",
      "Loss: 0.042769\n",
      "epoch: 333\n",
      "**********\n",
      "Loss: 0.042560\n",
      "epoch: 334\n",
      "**********\n",
      "Loss: 0.042351\n",
      "epoch: 335\n",
      "**********\n",
      "Loss: 0.042145\n",
      "epoch: 336\n",
      "**********\n",
      "Loss: 0.041941\n",
      "epoch: 337\n",
      "**********\n",
      "Loss: 0.041739\n",
      "epoch: 338\n",
      "**********\n",
      "Loss: 0.041538\n",
      "epoch: 339\n",
      "**********\n",
      "Loss: 0.041345\n",
      "epoch: 340\n",
      "**********\n",
      "Loss: 0.041144\n",
      "epoch: 341\n",
      "**********\n",
      "Loss: 0.040954\n",
      "epoch: 342\n",
      "**********\n",
      "Loss: 0.040758\n",
      "epoch: 343\n",
      "**********\n",
      "Loss: 0.040568\n",
      "epoch: 344\n",
      "**********\n",
      "Loss: 0.040380\n",
      "epoch: 345\n",
      "**********\n",
      "Loss: 0.040193\n",
      "epoch: 346\n",
      "**********\n",
      "Loss: 0.040009\n",
      "epoch: 347\n",
      "**********\n",
      "Loss: 0.039825\n",
      "epoch: 348\n",
      "**********\n",
      "Loss: 0.039649\n",
      "epoch: 349\n",
      "**********\n",
      "Loss: 0.039465\n",
      "epoch: 350\n",
      "**********\n",
      "Loss: 0.039291\n",
      "epoch: 351\n",
      "**********\n",
      "Loss: 0.039111\n",
      "epoch: 352\n",
      "**********\n",
      "Loss: 0.038941\n",
      "epoch: 353\n",
      "**********\n",
      "Loss: 0.038764\n",
      "epoch: 354\n",
      "**********\n",
      "Loss: 0.038597\n",
      "epoch: 355\n",
      "**********\n",
      "Loss: 0.038423\n",
      "epoch: 356\n",
      "**********\n",
      "Loss: 0.038260\n",
      "epoch: 357\n",
      "**********\n",
      "Loss: 0.038090\n",
      "epoch: 358\n",
      "**********\n",
      "Loss: 0.037925\n",
      "epoch: 359\n",
      "**********\n",
      "Loss: 0.037761\n",
      "epoch: 360\n",
      "**********\n",
      "Loss: 0.037604\n",
      "epoch: 361\n",
      "**********\n",
      "Loss: 0.037439\n",
      "epoch: 362\n",
      "**********\n",
      "Loss: 0.037285\n",
      "epoch: 363\n",
      "**********\n",
      "Loss: 0.037123\n",
      "epoch: 364\n",
      "**********\n",
      "Loss: 0.036972\n",
      "epoch: 365\n",
      "**********\n",
      "Loss: 0.036813\n",
      "epoch: 366\n",
      "**********\n",
      "Loss: 0.036664\n",
      "epoch: 367\n",
      "**********\n",
      "Loss: 0.036509\n",
      "epoch: 368\n",
      "**********\n",
      "Loss: 0.036359\n",
      "epoch: 369\n",
      "**********\n",
      "Loss: 0.036210\n",
      "epoch: 370\n",
      "**********\n",
      "Loss: 0.036062\n",
      "epoch: 371\n",
      "**********\n",
      "Loss: 0.035915\n",
      "epoch: 372\n",
      "**********\n",
      "Loss: 0.035774\n",
      "epoch: 373\n",
      "**********\n",
      "Loss: 0.035627\n",
      "epoch: 374\n",
      "**********\n",
      "Loss: 0.035484\n",
      "epoch: 375\n",
      "**********\n",
      "Loss: 0.035343\n",
      "epoch: 376\n",
      "**********\n",
      "Loss: 0.035203\n",
      "epoch: 377\n",
      "**********\n",
      "Loss: 0.035068\n",
      "epoch: 378\n",
      "**********\n",
      "Loss: 0.034926\n",
      "epoch: 379\n",
      "**********\n",
      "Loss: 0.034794\n",
      "epoch: 380\n",
      "**********\n",
      "Loss: 0.034655\n",
      "epoch: 381\n",
      "**********\n",
      "Loss: 0.034521\n",
      "epoch: 382\n",
      "**********\n",
      "Loss: 0.034388\n",
      "epoch: 383\n",
      "**********\n",
      "Loss: 0.034257\n",
      "epoch: 384\n",
      "**********\n",
      "Loss: 0.034126\n",
      "epoch: 385\n",
      "**********\n",
      "Loss: 0.033997\n",
      "epoch: 386\n",
      "**********\n",
      "Loss: 0.033868\n",
      "epoch: 387\n",
      "**********\n",
      "Loss: 0.033741\n",
      "epoch: 388\n",
      "**********\n",
      "Loss: 0.033614\n",
      "epoch: 389\n",
      "**********\n",
      "Loss: 0.033489\n",
      "epoch: 390\n",
      "**********\n",
      "Loss: 0.033365\n",
      "epoch: 391\n",
      "**********\n",
      "Loss: 0.033242\n",
      "epoch: 392\n",
      "**********\n",
      "Loss: 0.033120\n",
      "epoch: 393\n",
      "**********\n",
      "Loss: 0.032999\n",
      "epoch: 394\n",
      "**********\n",
      "Loss: 0.032878\n",
      "epoch: 395\n",
      "**********\n",
      "Loss: 0.032759\n",
      "epoch: 396\n",
      "**********\n",
      "Loss: 0.032641\n",
      "epoch: 397\n",
      "**********\n",
      "Loss: 0.032524\n",
      "epoch: 398\n",
      "**********\n",
      "Loss: 0.032408\n",
      "epoch: 399\n",
      "**********\n",
      "Loss: 0.032292\n",
      "epoch: 400\n",
      "**********\n",
      "Loss: 0.032178\n",
      "epoch: 401\n",
      "**********\n",
      "Loss: 0.032064\n",
      "epoch: 402\n",
      "**********\n",
      "Loss: 0.031952\n",
      "epoch: 403\n",
      "**********\n",
      "Loss: 0.031840\n",
      "epoch: 404\n",
      "**********\n",
      "Loss: 0.031729\n",
      "epoch: 405\n",
      "**********\n",
      "Loss: 0.031619\n",
      "epoch: 406\n",
      "**********\n",
      "Loss: 0.031510\n",
      "epoch: 407\n",
      "**********\n",
      "Loss: 0.031406\n",
      "epoch: 408\n",
      "**********\n",
      "Loss: 0.031294\n",
      "epoch: 409\n",
      "**********\n",
      "Loss: 0.031192\n",
      "epoch: 410\n",
      "**********\n",
      "Loss: 0.031082\n",
      "epoch: 411\n",
      "**********\n",
      "Loss: 0.030981\n",
      "epoch: 412\n",
      "**********\n",
      "Loss: 0.030874\n",
      "epoch: 413\n",
      "**********\n",
      "Loss: 0.030770\n",
      "epoch: 414\n",
      "**********\n",
      "Loss: 0.030672\n",
      "epoch: 415\n",
      "**********\n",
      "Loss: 0.030566\n",
      "epoch: 416\n",
      "**********\n",
      "Loss: 0.030469\n",
      "epoch: 417\n",
      "**********\n",
      "Loss: 0.030365\n",
      "epoch: 418\n",
      "**********\n",
      "Loss: 0.030269\n",
      "epoch: 419\n",
      "**********\n",
      "Loss: 0.030166\n",
      "epoch: 420\n",
      "**********\n",
      "Loss: 0.030073\n",
      "epoch: 421\n",
      "**********\n",
      "Loss: 0.029972\n",
      "epoch: 422\n",
      "**********\n",
      "Loss: 0.029880\n",
      "epoch: 423\n",
      "**********\n",
      "Loss: 0.029780\n",
      "epoch: 424\n",
      "**********\n",
      "Loss: 0.029689\n",
      "epoch: 425\n",
      "**********\n",
      "Loss: 0.029591\n",
      "epoch: 426\n",
      "**********\n",
      "Loss: 0.029501\n",
      "epoch: 427\n",
      "**********\n",
      "Loss: 0.029405\n",
      "epoch: 428\n",
      "**********\n",
      "Loss: 0.029313\n",
      "epoch: 429\n",
      "**********\n",
      "Loss: 0.029221\n",
      "epoch: 430\n",
      "**********\n",
      "Loss: 0.029129\n",
      "epoch: 431\n",
      "**********\n",
      "Loss: 0.029043\n",
      "epoch: 432\n",
      "**********\n",
      "Loss: 0.028950\n",
      "epoch: 433\n",
      "**********\n",
      "Loss: 0.028865\n",
      "epoch: 434\n",
      "**********\n",
      "Loss: 0.028773\n",
      "epoch: 435\n",
      "**********\n",
      "Loss: 0.028690\n",
      "epoch: 436\n",
      "**********\n",
      "Loss: 0.028599\n",
      "epoch: 437\n",
      "**********\n",
      "Loss: 0.028512\n",
      "epoch: 438\n",
      "**********\n",
      "Loss: 0.028431\n",
      "epoch: 439\n",
      "**********\n",
      "Loss: 0.028341\n",
      "epoch: 440\n",
      "**********\n",
      "Loss: 0.028261\n",
      "epoch: 441\n",
      "**********\n",
      "Loss: 0.028173\n",
      "epoch: 442\n",
      "**********\n",
      "Loss: 0.028094\n",
      "epoch: 443\n",
      "**********\n",
      "Loss: 0.028007\n",
      "epoch: 444\n",
      "**********\n",
      "Loss: 0.027929\n",
      "epoch: 445\n",
      "**********\n",
      "Loss: 0.027843\n",
      "epoch: 446\n",
      "**********\n",
      "Loss: 0.027766\n",
      "epoch: 447\n",
      "**********\n",
      "Loss: 0.027682\n",
      "epoch: 448\n",
      "**********\n",
      "Loss: 0.027606\n",
      "epoch: 449\n",
      "**********\n",
      "Loss: 0.027523\n",
      "epoch: 450\n",
      "**********\n",
      "Loss: 0.027444\n",
      "epoch: 451\n",
      "**********\n",
      "Loss: 0.027366\n",
      "epoch: 452\n",
      "**********\n",
      "Loss: 0.027288\n",
      "epoch: 453\n",
      "**********\n",
      "Loss: 0.027215\n",
      "epoch: 454\n",
      "**********\n",
      "Loss: 0.027134\n",
      "epoch: 455\n",
      "**********\n",
      "Loss: 0.027062\n",
      "epoch: 456\n",
      "**********\n",
      "Loss: 0.026982\n",
      "epoch: 457\n",
      "**********\n",
      "Loss: 0.026911\n",
      "epoch: 458\n",
      "**********\n",
      "Loss: 0.026832\n",
      "epoch: 459\n",
      "**********\n",
      "Loss: 0.026763\n",
      "epoch: 460\n",
      "**********\n",
      "Loss: 0.026685\n",
      "epoch: 461\n",
      "**********\n",
      "Loss: 0.026612\n",
      "epoch: 462\n",
      "**********\n",
      "Loss: 0.026543\n",
      "epoch: 463\n",
      "**********\n",
      "Loss: 0.026467\n",
      "epoch: 464\n",
      "**********\n",
      "Loss: 0.026400\n",
      "epoch: 465\n",
      "**********\n",
      "Loss: 0.026324\n",
      "epoch: 466\n",
      "**********\n",
      "Loss: 0.026257\n",
      "epoch: 467\n",
      "**********\n",
      "Loss: 0.026183\n",
      "epoch: 468\n",
      "**********\n",
      "Loss: 0.026118\n",
      "epoch: 469\n",
      "**********\n",
      "Loss: 0.026044\n",
      "epoch: 470\n",
      "**********\n",
      "Loss: 0.025980\n",
      "epoch: 471\n",
      "**********\n",
      "Loss: 0.025907\n",
      "epoch: 472\n",
      "**********\n",
      "Loss: 0.025839\n",
      "epoch: 473\n",
      "**********\n",
      "Loss: 0.025776\n",
      "epoch: 474\n",
      "**********\n",
      "Loss: 0.025704\n",
      "epoch: 475\n",
      "**********\n",
      "Loss: 0.025642\n",
      "epoch: 476\n",
      "**********\n",
      "Loss: 0.025571\n",
      "epoch: 477\n",
      "**********\n",
      "Loss: 0.025510\n",
      "epoch: 478\n",
      "**********\n",
      "Loss: 0.025440\n",
      "epoch: 479\n",
      "**********\n",
      "Loss: 0.025379\n",
      "epoch: 480\n",
      "**********\n",
      "Loss: 0.025311\n",
      "epoch: 481\n",
      "**********\n",
      "Loss: 0.025251\n",
      "epoch: 482\n",
      "**********\n",
      "Loss: 0.025183\n",
      "epoch: 483\n",
      "**********\n",
      "Loss: 0.025124\n",
      "epoch: 484\n",
      "**********\n",
      "Loss: 0.025057\n",
      "epoch: 485\n",
      "**********\n",
      "Loss: 0.024994\n",
      "epoch: 486\n",
      "**********\n",
      "Loss: 0.024936\n",
      "epoch: 487\n",
      "**********\n",
      "Loss: 0.024870\n",
      "epoch: 488\n",
      "**********\n",
      "Loss: 0.024813\n",
      "epoch: 489\n",
      "**********\n",
      "Loss: 0.024747\n",
      "epoch: 490\n",
      "**********\n",
      "Loss: 0.024691\n",
      "epoch: 491\n",
      "**********\n",
      "Loss: 0.024626\n",
      "epoch: 492\n",
      "**********\n",
      "Loss: 0.024571\n",
      "epoch: 493\n",
      "**********\n",
      "Loss: 0.024507\n",
      "epoch: 494\n",
      "**********\n",
      "Loss: 0.024448\n",
      "epoch: 495\n",
      "**********\n",
      "Loss: 0.024393\n",
      "epoch: 496\n",
      "**********\n",
      "Loss: 0.024331\n",
      "epoch: 497\n",
      "**********\n",
      "Loss: 0.024277\n",
      "epoch: 498\n",
      "**********\n",
      "Loss: 0.024215\n",
      "epoch: 499\n",
      "**********\n",
      "Loss: 0.024161\n",
      "epoch: 500\n",
      "**********\n",
      "Loss: 0.024100\n"
     ]
    }
   ],
   "source": [
    "train(twolayertransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be7d856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty', 'winters', 'shall', 'besiege'],\n",
       "  ['forty', 'winters', 'shall', 'besiege', 'thy']),\n",
       " (['forty', 'winters', 'shall', 'besiege', 'thy'],\n",
       "  ['winters', 'shall', 'besiege', 'thy', 'brow,']),\n",
       " (['winters', 'shall', 'besiege', 'thy', 'brow,'],\n",
       "  ['shall', 'besiege', 'thy', 'brow,', 'And']),\n",
       " (['shall', 'besiege', 'thy', 'brow,', 'And'],\n",
       "  ['besiege', 'thy', 'brow,', 'And', 'dig']),\n",
       " (['besiege', 'thy', 'brow,', 'And', 'dig'],\n",
       "  ['thy', 'brow,', 'And', 'dig', 'deep']),\n",
       " (['thy', 'brow,', 'And', 'dig', 'deep'],\n",
       "  ['brow,', 'And', 'dig', 'deep', 'trenches']),\n",
       " (['brow,', 'And', 'dig', 'deep', 'trenches'],\n",
       "  ['And', 'dig', 'deep', 'trenches', 'in']),\n",
       " (['And', 'dig', 'deep', 'trenches', 'in'],\n",
       "  ['dig', 'deep', 'trenches', 'in', 'thy']),\n",
       " (['dig', 'deep', 'trenches', 'in', 'thy'],\n",
       "  ['deep', 'trenches', 'in', 'thy', \"beauty's\"]),\n",
       " (['deep', 'trenches', 'in', 'thy', \"beauty's\"],\n",
       "  ['trenches', 'in', 'thy', \"beauty's\", 'field,'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bbb1c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['shall', 'besiege', 'thy', 'brow,', 'And'], predicted words are ['shall', 'besiege', 'thy', 'brow,', 'And']\n"
     ]
    }
   ],
   "source": [
    "evaluate(twolayertransformer, 2, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8bc186b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real words are ['brow,', 'And', 'dig', 'deep', 'trenches'], predicted words are ['brow,', 'And', 'dig', 'deep', 'trenches']\n"
     ]
    }
   ],
   "source": [
    "evaluate(twolayertransformer, 5, with_attention=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
